# --- PART 1: Imports, Global Configs, and Utility Caching ---
import streamlit as st
import pandas as pd
import numpy as np
import os, joblib
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
from arch import arch_model
from scipy.stats import genpareto, kurtosis, skew
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from keras.models import Sequential
from keras.layers import LSTM, Dropout, Dense
from keras.callbacks import EarlyStopping
from keras.models import load_model
from prophet import Prophet
from statsmodels.tsa.stattools import adfuller
import statsmodels.api as sm

ROLL_WINDOW = 60
ANNUALIZE = np.sqrt(252)
PCT_THRESHOLD = 0.95
EVT_TAIL_PCT = 0.995

# Page config
st.set_page_config(page_title="FX Volatility App", layout="wide")

# Background CSS
st.markdown("""
<style>
.stApp { background-color: black; }
</style>
""", unsafe_allow_html=True)

# --- FX Data Loader ---
@st.cache_data
def load_fx_data():
    df = pd.read_csv("reuters_fx_data.csv")
    df["Date"] = pd.to_datetime(df["Date"])
    df.sort_values(["Currency", "Date"], inplace=True)
    df["LogReturn"] = df.groupby("Currency")["Close"].transform(lambda x: np.log(x).diff())
    df["Volatility"] = df.groupby("Currency")["LogReturn"].transform(lambda x: x.rolling(ROLL_WINDOW).std()) * ANNUALIZE
    df["RealizedVol"] = df["OHLCVolatility"] / np.sqrt(252)
    return df.dropna()

# --- Sequence Creator for ML Models ---
def create_sequences(data, window):
    X, y = [], []
    for i in range(len(data) - window):
        X.append(data[i:i+window])
        y.append(data[i+window])
    return np.array(X), np.array(y)

# --- Model Loader ---
def load_models(currency_name, path="models"):
    try:
        rf = joblib.load(f"{path}/rf_{currency_name}.pkl")
        svr = joblib.load(f"{path}/svr_{currency_name}.pkl")
        lr = joblib.load(f"{path}/lr_{currency_name}.pkl")
        lstm = load_model(f"{path}/lstm_{currency_name}.h5", compile=False)
        scaler = joblib.load(f"{path}/scaler_{currency_name}.pkl")
        return {"Random Forest": rf, "SVR": svr, "Linear Regression": lr, "LSTM": lstm}, scaler
    except Exception as e:
        st.warning(f"Model loading failed for {currency_name}: {e}")
        return None, None

# --- Model Training and Saving ---
def train_and_save_models(currency_name, df, save_path="models", window=30):
    os.makedirs(save_path, exist_ok=True)
    df_currency = df[df["Currency"] == currency_name][["Date", "Close"]].dropna().copy()
    df_currency["Date"] = pd.to_datetime(df_currency["Date"])
    df_currency.set_index("Date", inplace=True)

    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(df_currency)

    X, y = create_sequences(scaled_data, window)
    X_rf = X.reshape(X.shape[0], -1)
    X_lstm = X.reshape(X.shape[0], X.shape[1], 1)
    y = y.ravel()

    rf_params = {"n_estimators": [50, 100], "max_depth": [5, 10, None]}
    rf = GridSearchCV(RandomForestRegressor(random_state=42), rf_params, cv=TimeSeriesSplit(n_splits=3))
    rf.fit(X_rf, y)
    joblib.dump(rf.best_estimator_, f"{save_path}/rf_{currency_name}.pkl")

    svr_params = {"C": [1, 10], "gamma": ["scale", "auto"], "epsilon": [0.01, 0.1]}
    svr = GridSearchCV(SVR(), svr_params, cv=TimeSeriesSplit(n_splits=3))
    svr.fit(X_rf, y)
    joblib.dump(svr.best_estimator_, f"{save_path}/svr_{currency_name}.pkl")

    lr = LinearRegression()
    lr.fit(X_rf, y)
    joblib.dump(lr, f"{save_path}/lr_{currency_name}.pkl")

    lstm = Sequential()
    lstm.add(LSTM(64, input_shape=(X_lstm.shape[1], X_lstm.shape[2])))
    lstm.add(Dropout(0.2))
    lstm.add(Dense(1))
    lstm.compile(loss='mse', optimizer='adam')

    early_stop = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)
    lstm.fit(X_lstm, y, epochs=20, batch_size=16, verbose=0, callbacks=[early_stop])
    lstm.save(f"{save_path}/lstm_{currency_name}.h5")

    joblib.dump(scaler, f"{save_path}/scaler_{currency_name}.pkl")
    return True

# --- FX Dataset ---
df = load_fx_data()

# --- Sidebar UI ---
st.sidebar.header("Train FX Forecasting Models")
currency_option = st.sidebar.selectbox("Select Currency", df["Currency"].unique().tolist())
if st.sidebar.button("Train & Save Models"):
    with st.spinner(f"Training models for {currency_option}..."):
        success = train_and_save_models(currency_option, df)
    if success:
        st.sidebar.success(f"Models for {currency_option} saved to /models/")


# Assign manual volatility group
def assign_manual_group(vol):
    if vol < 0.07:
        return "Group 1"
    elif vol < 0.50:
        return "Group 2"
    elif vol < 0.60:
        return "Group 3"
    else:
        return "Group 4"

# --- PART 2: Threshold Summary (with Caching) and Prophet Optimization ---

@st.cache_data
def compute_thresholds_per_currency(df):
    summary = []
    for ccy, group in df.groupby("Currency"):
        # Volatility for dynamic models (unchanged)
        vol_series = group["Volatility"].dropna()
        avg_vol = vol_series.mean()

        # Manual group uses realized OHLC volatility
        realized_series = group["RealizedVol"].dropna()
        avg_realized_vol = realized_series.mean()

        # Manual group and threshold (based on OHLC-based logic)
        def assign_manual_group(vol):
            if vol < 0.07:
                return "Group 1"
            elif vol < 0.50:
                return "Group 2"
            elif vol < 0.60:
                return "Group 3"
            else:
                return "Group 4"

        manual_group = assign_manual_group(avg_realized_vol)
        manual_threshold = {
            "Group 1": 0.10,
            "Group 2": 0.25,
            "Group 3": 0.55,
            "Group 4": 0.80
        }[manual_group]

        try:
            am = arch_model(vol_series, vol='GARCH', p=1, q=1)
            res = am.fit(disp="off")
            forecast = res.forecast(horizon=1).variance.values[-1][0] ** 0.5
        except:
            forecast = np.nan

        tail_data = vol_series[vol_series > vol_series.quantile(EVT_TAIL_PCT)]
        try:
            evt_params = genpareto.fit(tail_data)
            evt_threshold = genpareto.ppf(0.999, *evt_params)
        except:
            evt_threshold = np.nan

        summary.append({
            "Currency": ccy,
            "AvgVol": avg_vol,
            "AvgRealizedVol": avg_realized_vol,
            "ManualGroup": manual_group,
            "ManualThreshold": manual_threshold,
            "GARCH_Forecast": forecast,
            "95th_Pct": vol_series.quantile(PCT_THRESHOLD),
            "EVT_Threshold": evt_threshold
        })
    return pd.DataFrame(summary)

df_summary = compute_thresholds_per_currency(df)

# --- Stationarity Check ---
def adf_stationarity_test(series):
    result = adfuller(series.dropna())
    return {
        "ADF Statistic": result[0],
        "p-value": result[1],
        "IsStationary": result[1] < 0.05
    }

# --- Prophet Forecast (Univariate) ---
@st.cache_data
def prophet_forecast_univariate(currency_data):
    df_prophet = currency_data[["Date", "Close"]].dropna().rename(columns={"Date": "ds", "Close": "y"})
    model = Prophet(seasonality_mode="multiplicative", daily_seasonality=True)
    model.fit(df_prophet)
    future = model.make_future_dataframe(periods=15)
    forecast = model.predict(future)
    return forecast, model

# --- Prophet Forecast (Multivariate) ---
@st.cache_data
def prophet_forecast_multivariate(currency_data):
    df_prophet = currency_data[["Date", "Close", "High", "Low"]].dropna().rename(
        columns={"Date": "ds", "Close": "y", "High": "add1", "Low": "add2"})
    model = Prophet(seasonality_mode="multiplicative", daily_seasonality=True)
    model.add_regressor("add1")
    model.add_regressor("add2")
    model.fit(df_prophet)
    future = df_prophet.drop(columns=["y"])
    forecast = model.predict(future)
    return forecast, model

# --- Seasonal Decomposition ---
def seasonal_decomposition_plot(df, currency, freq="W"):
    sub_df = df[df["Currency"] == currency][["Date", "Close"]].dropna()
    series = sub_df.set_index("Date")["Close"].resample(freq).mean().ffill()

    min_required = {"W": 104, "D": 730, "M": 24}[freq]
    if len(series) < min_required:
        st.warning(f"Not enough data for {freq} decomposition (found {len(series)}, need â‰¥ {min_required})")
        return

    result = sm.tsa.seasonal_decompose(series, model='multiplicative')
    fig = result.plot()
    plt.suptitle(f"{currency} Seasonal Decomposition ({freq})")
    st.pyplot(fig)
# --- PART 3: Tab 1 (Overview) and Tab 2 (Threshold Viewer) ---

# Tab layout
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "Overview", "Thresholds", "Live FX Rate Forecasting",
    "Final Results & Recommendation", "Cross-Currency View"
])

with tab1:
    st.title("FX Volatility Thresholding: Manual vs. Dynamic ML Approach")

    st.subheader(" Why Monitor Volatility Thresholds in FX Trading?")
    st.markdown("""
    In institutional FX markets, every trade is subject to pricing scrutiny. To flag **potentially off-market trades**, teams use volatility-based thresholds.

    These thresholds define what is **acceptable deviation** from a benchmark market rate.

    **Key users:**
    - **Operations teams**: Auto-flag deviations for review
    - **Compliance**: Ensure fair pricing and regulatory defensibility
    - **Quants**: Monitor market regime changes or anomalies
    """)

    st.subheader(" Manual Thresholding: The Current Baseline")
    st.markdown("""
    The existing operations process applies **fixed volatility thresholds** per currency using the following logic:
    - Calculate average *OHLC-based volatility*
    - Bucket currencies into **4 static groups**
        - Group 1: 0.00â€“0.07 â†’ Threshold = 0.10  
        - Group 2: 0.07â€“0.50 â†’ Threshold = 0.25  
        - Group 3: 0.50â€“0.60 â†’ Threshold = 0.55  
        - Group 4: >0.60 â†’ Threshold = 0.80  
    - Thresholds are **revised manually every few quarters** or post-audit

    **Problems with this approach:**
    -  Thresholds don't adapt to **market regime shifts**
    -  Doesn't capture volatility **clustering**, **tail risk**, or **skew**
    -  No visibility into **statistical confidence** or **model-based forecasting**
    -  Limited defensibility in regulatory or audit scenarios
    """)

    st.subheader(" Our Solution: Dynamic, Model-Aware, Explainable Thresholds")
    st.markdown("""
    We combine **multiple statistical and machine learning techniques** to calculate per-currency volatility thresholds weekly:

    **Statistical Models:**
    - **GARCH**: Captures volatility clustering and persistence
    - **EVT (Extreme Value Theory)**: Quantifies tail risk in extreme deviations
    - **Rolling 95th percentile**: Baseline statistical threshold

    **Machine Learning Models:**
    - Trained per currency (e.g., INR, JPY) using LSTM, Random Forest, SVR
    - Computes **forecast error bands** to identify probable breaches
    - Tracks **MAPE** to dynamically adjust thresholds based on prediction reliability

    **Final Threshold Decision**:
    - Take the **maximum** of GARCH, EVT, 95th percentile
    - Overlay **forecast confidence bands**
    - Compare with manual thresholds for justification

    **Business Benefits**:
    -  **Explains every alert** (e.g., "flagged due to LSTM band breach + high EVT tail risk")
    -  **Weekly recalibration** supports real-time regimes
    -  Builds **trust** through transparency (flags justified with data)
    -  Aligns Ops + Quants + Compliance under a unified model
    """)

    st.success("ðŸ‘‰ Use the tabs above to explore how thresholds are computed, visualized, and compared across currencies and methods.")


# --- Tab 2: Threshold Comparison ---
with tab2:
    st.header("Threshold Comparison Summary")

    st.dataframe(df_summary[[
        "Currency", "AvgVol", "AvgRealizedVol", "ManualGroup",
        "ManualThreshold", "GARCH_Forecast", "95th_Pct", "EVT_Threshold"
    ]], use_container_width=True)

    sel = st.selectbox("Select Currency", df_summary["Currency"], key="tab2_currency")
    series = df[df["Currency"] == sel].copy()
    summary = df_summary[df_summary["Currency"] == sel].iloc[0]

    fig = px.line(series, x="Date", y="Volatility", title=f"{sel} Volatility & Thresholds (LogReturn-based)")
    for label in ["ManualThreshold", "GARCH_Forecast", "95th_Pct", "EVT_Threshold"]:
        val = summary[label]
        if pd.notnull(val):
            fig.add_hline(y=val, line_dash="dot", annotation_text=label)

    # Overlay Ops-style volatility (Realized)
    fig.add_trace(go.Scatter(
        x=series["Date"], y=series["RealizedVol"],
        name="RealizedVol (Ops-style)", line=dict(dash="dash", color="orange")
    ))

    st.plotly_chart(fig, use_container_width=True)

    st.markdown(f"""
    **Notes**:
    - `Volatility`: Based on log-returns (used in GARCH, EVT, 95th).
    - `RealizedVol`: Based on OHLC, used by Ops for manual grouping.
    - `ManualThreshold`: Based on `RealizedVol` average.
    """)


# --- Tab 3: Forecast with Dynamic Test Period ---
with tab3:
    st.header("Live FX Forecasting (Flexible Test Period)")

    selected_currency = st.selectbox("Select Currency", df["Currency"].unique(),index=None, key="fx_forecast_currency")
    test_days = st.slider("Select Test Period (days before latest date)", min_value=30, max_value=90, step=30, value=60)

    if selected_currency:
        df_ccy = df[df["Currency"] == selected_currency][["Date", "Close"]].dropna().copy()
        df_ccy["Date"] = pd.to_datetime(df_ccy["Date"])
        df_ccy.set_index("Date", inplace=True)

        latest_date = df_ccy.index.max()
        test_start = latest_date - pd.Timedelta(days=test_days)
        train_df = df_ccy[df_ccy.index < test_start]
        test_df = df_ccy[df_ccy.index >= test_start]

        if train_df.empty or test_df.empty:
            st.error("Not enough data for selected test period.")
        else:
            scaler = MinMaxScaler()
            scaled_all = scaler.fit_transform(df_ccy)
            scaled_train = scaler.transform(train_df)
            scaled_test = scaler.transform(test_df)

            def create_sequences(data, window):
                X, y = [], []
                for i in range(len(data) - window):
                    X.append(data[i:i+window])
                    y.append(data[i+window])
                return np.array(X), np.array(y)

            WINDOW = 30
            X_train, y_train = create_sequences(scaled_train, WINDOW)
            X_rf_test = []
            for i in range(len(test_df)):
                X_rf_test.append(scaled_all[-(len(test_df) + WINDOW - i):-len(test_df) + i])
            X_rf_test = np.array(X_rf_test).reshape(len(X_rf_test), -1)
            X_train_rf = X_train.reshape(X_train.shape[0], -1)
            X_train_lstm = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_lstm_test = np.array([x.reshape(WINDOW, 1) for x in X_rf_test])

            model_outputs_train, model_outputs_test = {}, {}
            rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
            rf.fit(X_train_rf, y_train.ravel())
            model_outputs_train["Random Forest"] = rf.predict(X_train_rf)
            model_outputs_test["Random Forest"] = rf.predict(X_rf_test)

            lstm = Sequential()
            lstm.add(LSTM(64, input_shape=(WINDOW, 1)))
            lstm.add(Dropout(0.2))
            lstm.add(Dense(1))
            lstm.compile(optimizer="adam", loss="mse")
            lstm.fit(X_train_lstm, y_train, epochs=20, batch_size=16, verbose=0)
            model_outputs_train["LSTM"] = lstm.predict(X_train_lstm).ravel()
            model_outputs_test["LSTM"] = lstm.predict(X_lstm_test).ravel()

            lr = LinearRegression()
            lr.fit(X_train_rf, y_train)
            model_outputs_train["Linear Regression"] = lr.predict(X_train_rf)
            model_outputs_test["Linear Regression"] = lr.predict(X_rf_test)

            y_true_train = scaler.inverse_transform(y_train.reshape(-1, 1)).flatten()
            y_true_test = test_df["Close"].values

            fig = go.Figure()
            fig.add_trace(go.Scatter(x=train_df.index[-len(y_true_train):], y=y_true_train, name="Actual (Train)", line=dict(color="white")))
            fig.add_trace(go.Scatter(x=test_df.index, y=y_true_test, name="Actual (Test)", line=dict(color="white", dash="dot")))

            metrics = []
            for model, preds_train in model_outputs_train.items():
                preds_train_inv = scaler.inverse_transform(preds_train.reshape(-1, 1)).flatten()
                preds_test_inv = scaler.inverse_transform(model_outputs_test[model].reshape(-1, 1)).flatten()

                fig.add_trace(go.Scatter(x=train_df.index[-len(y_true_train):], y=preds_train_inv, name=f"{model} (Train)", line=dict(dash="dash")))
                fig.add_trace(go.Scatter(x=test_df.index, y=preds_test_inv, name=f"{model} (Test)", line=dict(dash="solid")))

                metrics.append({
                    "Model": model,
                    "Train MAPE": mean_absolute_percentage_error(y_true_train, preds_train_inv) * 100,
                    "Test MAPE": mean_absolute_percentage_error(y_true_test, preds_test_inv) * 100,
                    "Train RMSE": np.sqrt(mean_squared_error(y_true_train, preds_train_inv)),
                    "Test RMSE": np.sqrt(mean_squared_error(y_true_test, preds_test_inv))
                })

            best_model = min(metrics, key=lambda x: x["Test MAPE"])
            model_name = best_model["Model"]
            best_mape = best_model["Test MAPE"]

            # ðŸ” Log best MAPE per currency
            log_entry = pd.DataFrame([{
                "RunDate": pd.Timestamp.today(),
                "Currency": selected_currency,
                "Model": model_name,
                "MAPE": best_mape
            }])
            log_path = "model_feedback_log.csv"
            if os.path.exists(log_path):
                existing = pd.read_csv(log_path)
                full_log = pd.concat([existing, log_entry], ignore_index=True)
            else:
                full_log = log_entry
            full_log.to_csv(log_path, index=False)

            preds_inv = scaler.inverse_transform(model_outputs_test[model_name].reshape(-1, 1)).flatten()
            mape_pct = best_mape / 100
            upper_bound = preds_inv * (1 + mape_pct)
            lower_bound = preds_inv * (1 - mape_pct)
            flag_mask = (y_true_test > upper_bound) | (y_true_test < lower_bound)

            fig.add_trace(go.Scatter(x=test_df.index, y=upper_bound, name="Dynamic Upper", line=dict(dash="dot", color="green")))
            fig.add_trace(go.Scatter(x=test_df.index, y=lower_bound, name="Dynamic Lower", line=dict(dash="dot", color="green")))
            fig.add_trace(go.Scatter(
                x=test_df.index[flag_mask],
                y=y_true_test[flag_mask],
                mode="markers",
                name="Flagged Deviations",
                marker=dict(color="red", size=10, symbol="x")
            ))

            fig.update_layout(
                title=f"{selected_currency}: Forecast vs Thresholds ({model_name})",
                xaxis_title="Date",
                yaxis_title="FX Rate"
            )
            st.plotly_chart(fig, use_container_width=True)


# --- Tab 4: Final Thresholds & Forecast Accuracy ---
with tab4:
    st.header("Final Thresholds & Forecast Accuracy")

    df_summary["FinalThreshold"] = df_summary[[ 
        "ManualThreshold", "95th_Pct", "GARCH_Forecast", "EVT_Threshold"
    ]].max(axis=1)

    @st.cache_data
    def load_best_mape_summary():
        try:
            log_df = pd.read_csv("model_feedback_log.csv")
            log_df["Currency"] = log_df["Currency"].str.strip().str.upper()
            df_summary["Currency"] = df_summary["Currency"].str.strip().str.upper()

            # Keep most recent run per currency
            latest = log_df.sort_values("RunDate", ascending=False).drop_duplicates(subset=["Currency"])
            best_model = latest.groupby("Currency")[["MAPE", "Model"]].first().reset_index()
            best_model.rename(columns={"MAPE": "Best_MAPE"}, inplace=True)
            return best_model
        except Exception as e:
            st.error(f"Error loading MAPE summary: {e}")
            return pd.DataFrame(columns=["Currency", "Best_MAPE", "Model"])

    best_mape_df = load_best_mape_summary()
    df_final = df_summary.merge(best_mape_df, on="Currency", how="left")

    selected_currency = st.selectbox("Select Currency", df_final["Currency"], key="tab4_currency")
    row = df_final[df_final["Currency"] == selected_currency].iloc[0]

    st.subheader(f"{selected_currency}: Thresholds & Justification")
    st.markdown(f"""
    - **Avg Log-Return Volatility** (Dynamic): `{round(row['AvgVol'], 4)}`
    - **Avg OHLC Realized Volatility** (Manual): `{round(row['AvgRealizedVol'], 4)}`
    - **Manual Group (based on OHLC)**: `{row['ManualGroup']}`
    - **Manual Threshold**: `{round(row['ManualThreshold'], 4)}`
    - **Final Dynamic Threshold** (Max of GARCH, EVT, 95th): `{round(row['FinalThreshold'], 4)}`
    - **Best Model MAPE**: `{round(row['Best_MAPE'], 2) if pd.notnull(row['Best_MAPE']) else 'NA'}%`
    - **Best Forecasting Model**: `{row['Model'] if pd.notnull(row['Model']) else 'NA'}`
    """)

    fig = go.Figure()
    fig.add_trace(go.Bar(name="Manual", x=[selected_currency], y=[row["ManualThreshold"]]))
    fig.add_trace(go.Bar(name="Final Dynamic", x=[selected_currency], y=[row["FinalThreshold"]]))
    if pd.notnull(row["Best_MAPE"]):
        fig.add_trace(go.Bar(name="Best Model MAPE", x=[selected_currency], y=[row["Best_MAPE"] / 100]))

    fig.update_layout(title="Threshold Comparison", barmode="group")
    st.plotly_chart(fig, use_container_width=True)

    # Volatility trend + breach bands
    st.subheader("Historical Volatility Trend & Breach Flags")
    trend_df = df[df["Currency"] == selected_currency].copy()
    trend_df = trend_df.sort_values("Date")
    y_actual = trend_df["Volatility"].values
    dates = trend_df["Date"]

    mape_pct = row["Best_MAPE"] / 100 if pd.notnull(row["Best_MAPE"]) else 0.05
    upper_band = y_actual * (1 + mape_pct)
    lower_band = y_actual * (1 - mape_pct)
    flag_mask = (y_actual > upper_band) | (y_actual < lower_band)

    fig_vol = go.Figure()
    fig_vol.add_trace(go.Scatter(x=dates, y=y_actual, name="Actual Volatility", line=dict(color="white")))
    fig_vol.add_trace(go.Scatter(x=dates, y=upper_band, name="Dynamic Upper Band", line=dict(dash="dot", color="green")))
    fig_vol.add_trace(go.Scatter(x=dates, y=lower_band, name="Dynamic Lower Band", line=dict(dash="dot", color="green")))

    fig_vol.add_hline(y=row["ManualThreshold"], line_color="red", line_dash="dot", annotation_text="Manual")
    fig_vol.add_hline(y=row["FinalThreshold"], line_color="orange", line_dash="dash", annotation_text="Final Dynamic")

    fig_vol.add_trace(go.Scatter(
        x=dates[flag_mask],
        y=y_actual[flag_mask],
        mode="markers",
        name="Flagged Deviations",
        marker=dict(color="red", size=8, symbol="x")
    ))

    fig_vol.update_layout(
        title=f"{selected_currency}: Volatility with Manual, Dynamic Thresholds & Flags",
        xaxis_title="Date",
        yaxis_title="Annualized Volatility"
    )
    st.plotly_chart(fig_vol, use_container_width=True)


# --- Tab 5: Cross-Currency Threshold Estimation ---
with tab5:
    st.header("Cross-Currency Thresholds (Synthetic)")

    st.markdown("""
    This module calculates cross-currency thresholds (e.g., AUDCZK = AUDUSD Ã— USDCZK) using historical data.
    - Dynamic threshold = 95th percentile of synthetic volatility.
    - Manual threshold = max(manual thresholds of base and quote legs).
    - Regime flags and threshold breaches shown below.
    """)

    currencies = df["Currency"].unique().tolist()
    cross_pairs = [(f"{b}{q}", b, q) for b in currencies for q in currencies if b != q]
    pair_labels = [f"{p[0]} = {p[1]}USD Ã— USD{p[2]}" for p in cross_pairs]
    pair_choice = st.selectbox("Select Cross Pair", pair_labels)

    selected_pair = cross_pairs[pair_labels.index(pair_choice)]
    pair_name, base_ccy, quote_ccy = selected_pair

    try:
        base_df = df[df["Currency"] == base_ccy][["Date", "Close"]].copy()
        quote_df = df[df["Currency"] == quote_ccy][["Date", "Close"]].copy()

        merged = pd.merge(base_df, quote_df, on="Date", suffixes=(f"_{base_ccy}", f"_{quote_ccy}")).dropna()
        merged["SyntheticRate"] = merged[f"Close_{base_ccy}"] / merged[f"Close_{quote_ccy}"]
        merged["LogReturn"] = np.log(merged["SyntheticRate"]).diff()
        merged["CrossVolatility"] = merged["LogReturn"].rolling(ROLL_WINDOW).std() * ANNUALIZE
        merged.dropna(inplace=True)

        df_thresh = df_summary.set_index("Currency")
        manual_cross = max(df_thresh.loc[base_ccy, "ManualThreshold"], df_thresh.loc[quote_ccy, "ManualThreshold"])
        dynamic_cross = merged["CrossVolatility"].quantile(PCT_THRESHOLD)

        # Breach flags
        merged["Flag"] = merged["CrossVolatility"] > dynamic_cross

        # Plot
        st.subheader(f"{pair_name} Thresholds")
        st.markdown(f"""
        - **Cross Volatility Range**: `{merged['CrossVolatility'].min():.4f}` to `{merged['CrossVolatility'].max():.4f}`
        - **Manual Cross Threshold**: `{manual_cross:.4f}`
        - **Dynamic Cross Threshold (95th pct)**: `{dynamic_cross:.4f}`
        """)

        fig = px.line(merged, x="Date", y="CrossVolatility", title=f"{pair_name} Volatility (Synthetic)")
        fig.add_hline(y=manual_cross, line_color="red", line_dash="dot", annotation_text="Manual")
        fig.add_hline(y=dynamic_cross, line_color="green", line_dash="dash", annotation_text="Dynamic")

        # Add breach flags
        breach_points = merged[merged["Flag"] == True]
        fig.add_trace(go.Scatter(
            x=breach_points["Date"],
            y=breach_points["CrossVolatility"],
            mode="markers",
            name="Threshold Breach",
            marker=dict(color="orange", size=10, symbol="x")
        ))

        st.plotly_chart(fig, use_container_width=True)

    except Exception as e:
        st.error(f"Failed to compute synthetic cross pair volatility: {e}")
