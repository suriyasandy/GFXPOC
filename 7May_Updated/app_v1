import streamlit as st
import pandas as pd
import re,os
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
from arch import arch_model
from scipy.stats import genpareto, kurtosis, skew
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from keras.models import Sequential
from keras.layers import LSTM, Dropout, Dense
from prophet import Prophet

ROLL_WINDOW = 60
ANNUALIZE = np.sqrt(252)
PCT_THRESHOLD = 0.95
EVT_TAIL_PCT = 0.995
# Set page config
st.set_page_config(page_title="FX Volatility App", layout="wide")

# Inject custom CSS for background color
st.markdown(
    """
    <style>
    .stApp {
        background-color: black; /* Light grey-blue background */
    }
    </style>
    """,
    unsafe_allow_html=True
)

# Example UI element
st.title("FX Volatility Monitoring App")
@st.cache_data
def load_fx_data():
    df = pd.read_csv("reuters_fx_data.csv")
    df["Date"] = pd.to_datetime(df["Date"])
    df.sort_values(["Currency", "Date"], inplace=True)
    df["LogReturn"] = df.groupby("Currency")["Close"].transform(lambda x: np.log(x).diff())
    df["Volatility"] = df.groupby("Currency")["LogReturn"].transform(lambda x: x.rolling(ROLL_WINDOW).std()) * ANNUALIZE
    return df.dropna()

df = load_fx_data()

def assign_manual_group(vol):
    if vol < 0.07:
        return "Group 1"
    elif vol < 0.50:
        return "Group 2"
    elif vol < 0.60:
        return "Group 3"
    else:
        return "Group 4"

def compute_thresholds_per_currency(df):
    summary = []
    for ccy, group in df.groupby("Currency"):
        vol_series = group["Volatility"].dropna()
        avg_vol = vol_series.mean()
        manual_group = assign_manual_group(avg_vol)
        manual_threshold = {"Group 1": 0.10, "Group 2": 0.25, "Group 3": 0.55, "Group 4": 0.80}[manual_group]

        am = arch_model(vol_series, vol='GARCH', p=1, q=1)
        res = am.fit(disp="off")
        forecast = res.forecast(horizon=1).variance.values[-1][0] ** 0.5

        tail_data = vol_series[vol_series > vol_series.quantile(EVT_TAIL_PCT)]
        evt_params = genpareto.fit(tail_data)
        evt_threshold = genpareto.ppf(0.999, *evt_params)

        smile_rr = skew(group["LogReturn"].dropna()) * 0.02
        smile_bf = (kurtosis(group["LogReturn"].dropna()) - 3) * 0.01
        atm = avg_vol
        smile_threshold = atm + 1.0 * smile_rr + 1.0 * smile_bf

        summary.append({
            "Currency": ccy,
            "AvgVol": avg_vol,
            "ManualGroup": manual_group,
            "ManualThreshold": manual_threshold,
            "GARCH_Forecast": forecast,
            "95th_Pct": vol_series.quantile(PCT_THRESHOLD),
            "EVT_Threshold": evt_threshold
            #"ATM": atm,
            #"RR": smile_rr,
            #"BF": smile_bf,
            #"Smile_Threshold": smile_threshold
            
        })
    return pd.DataFrame(summary)

df_summary = compute_thresholds_per_currency(df)

# UI layout
tab1, tab2, tab3, tab4 = st.tabs(["Overview", "Thresholds","Recalibration Simulation","Live FX Rate Forecasting"])
with tab1:
    st.title("FX Volatility Thresholding: Manual vs Dynamic")

    st.subheader(" Business Use Case")
    st.markdown("""
    In institutional FX markets, trades are monitored for **rate deviations** from market benchmarks.
    A key component of this is **volatility thresholding**, which helps:
    - Flag trades with excessive deviation.
    - Ensure fairness, reduce risk, and maintain compliance.

    Currently, operations teams use **static thresholds** per currency, based on manual grouping logic.
    But these thresholds:
    - Donâ€™t adapt to changing market conditions.
    - Fail to account for structural behaviors like **volatility clustering** or **option smiles**.
    """)

    st.subheader(" Manual Approach (Baseline)")
    st.markdown("""
    - Currencies are bucketed into 4 fixed groups (based on average volatility ranges).
    - Each group has a fixed threshold (e.g., 0.10, 0.25...).
    - These thresholds are static and reviewed annually or post-incident.

    **Limitations**:
    - Outdated during regime shifts.
    - Not tailored to individual currency behavior.
    - No support for cross-currency or structural features like skew or RR/BF.
    """)

    st.subheader(" Our Dynamic Approach")
    st.markdown("""
    We simulate and compare **multiple thresholding methods**:
    -  GARCH: Captures volatility clustering.
    -  EVT: Captures extreme tail risks.
    -  Rolling statistics & 95th percentile comparisons.

    All results are:
    - Transparent & visual.
    - Easy to interpret.
    - Designed for **weekly recalibration** instead of static thresholds.
    """)

    st.success("Use the tabs above to compare manual thresholds with smarter, data-driven models!")


with tab2:
    st.header("Threshold Comparison")
    st.dataframe(df_summary, use_container_width=True)

    sel = st.selectbox("Select Currency", df["Currency"].unique(), key="tab2_currency")
    series = df[df["Currency"] == sel]
    summary = df_summary[df_summary["Currency"] == sel].iloc[0]

    fig = px.line(series, x="Date", y="Volatility", title=f"{sel} Volatility & Thresholds")
    for label in ["ManualThreshold", "GARCH_Forecast", "95th_Pct", "EVT_Threshold"]:
        fig.add_hline(y=summary[label], line_dash="dot", annotation_text=label)
    st.plotly_chart(fig, use_container_width=True)

with tab3:
    st.header("Real-Time Recalibration Simulation")

    subtab1, subtab2 = st.tabs(["Recalibrated Thresholds Summary", "Currency Trend View"])

    with subtab1:
        st.markdown("""
        This module simulates post-14 Oct 2024 FX market shocks for selected currencies and shows how dynamic models recalibrate thresholds automatically,
        while manual thresholds remain static.
        """)

        # User inputs
        shock_end_date = st.date_input("Select Shock End Date", pd.to_datetime("2024-11-01"))
        roll = st.slider("Rolling Window", 10, 90, ROLL_WINDOW)

        # Generate synthetic shocks
        fx_df = df.copy()
        impacted_ccys = ["INR", "JPY", "GBP"]
        test_rows = []

        for ccy in fx_df["Currency"].unique():
            ccy_df = fx_df[fx_df["Currency"] == ccy].copy()
            last_date = ccy_df["Date"].max()
            start = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=1, freq='B')[0]
            end = pd.to_datetime(shock_end_date)

            if start >= end:
                continue

            date_range = pd.date_range(start=start, end=end, freq='B')
            shock_factor = 1.5 if ccy in impacted_ccys else 1.0

            last_close = ccy_df["Close"].iloc[-1]
            for dt in date_range:
                new_close = last_close * np.random.normal(loc=1.0, scale=0.01)
                test_rows.append([ccy, dt, new_close])

        test_df = pd.DataFrame(test_rows, columns=["Currency", "Date", "Close"])
        combined = pd.concat([df[["Currency", "Date", "Close"]], test_df])
        combined.sort_values(["Currency", "Date"], inplace=True)
        combined["LogReturn"] = combined.groupby("Currency")["Close"].transform(lambda x: np.log(x).diff())
        combined["Volatility"] = combined.groupby("Currency")["LogReturn"].transform(lambda x: x.rolling(roll).std()) * ANNUALIZE
        combined.dropna(inplace=True)
        combined.to_csv("reuters_gfx_test_data.csv", index=False)

        st.success("Generated `reuters_gfx_test_data.csv` with shocked volatilities.")

        # Recalculate thresholds post shock
        post_df = combined[combined["Date"] > pd.to_datetime("2024-10-14")]
        dynamic = post_df.groupby("Currency").agg(
            AvgVol_Shocked=("Volatility", "mean"),
            New95thPct=("Volatility", lambda x: x.quantile(PCT_THRESHOLD)),
            NewEVT=("Volatility", lambda x: genpareto.ppf(0.999, *genpareto.fit(x[x > x.quantile(EVT_TAIL_PCT)])))
        ).reset_index()

        recalibrated = df_summary[["Currency", "ManualThreshold"]].merge(dynamic, on="Currency", how="inner")
        st.subheader("Recalibrated Thresholds Using Test Data")
        st.dataframe(recalibrated, use_container_width=True)

        fig = px.bar(recalibrated.melt(id_vars="Currency", value_vars=["ManualThreshold", "New95thPct", "NewEVT"]),
                     x="Currency", y="value", color="variable", barmode="group",
                     title="Manual vs Recalibrated Thresholds (Post-Volatility Shock)",
                     labels={"value": "Threshold", "variable": "Method"})
        st.plotly_chart(fig, use_container_width=True)

    with subtab2:
        st.subheader("Trend Chart with Shock Impact")
        ccy_choice = st.selectbox("Select Currency to View Trend", recalibrated["Currency"].unique() if not recalibrated.empty else [])

        if ccy_choice:
            plot_df = combined[combined["Currency"] == ccy_choice]
            thresholds = recalibrated[recalibrated["Currency"] == ccy_choice]

            fig = px.line(plot_df, x="Date", y="Volatility", title=f"{ccy_choice} Volatility with Shock & Thresholds")
            fig.add_hline(y=thresholds["ManualThreshold"].values[0], line_dash="dot", annotation_text="Manual")
            fig.add_hline(y=thresholds["New95thPct"].values[0], line_dash="dot", annotation_text="New 95th Pct")
            fig.add_hline(y=thresholds["NewEVT"].values[0], line_dash="dot", annotation_text="New EVT")
            st.plotly_chart(fig, use_container_width=True)

with tab4:
    st.header("Live FX Rate Forecasting")

    tab4_sub1, tab4_sub2, tab4_sub3 = st.tabs(["Base Forecasting", "Shocked Forecasting","FeedbackLoop Metrics"])
    def run_forecast(df_input,suffix):
        thresholds = recalibrated[recalibrated["Currency"] == selected_currency]
        df_ccy = df_input[df_input["Currency"] == selected_currency].copy()
        df_ccy = df_ccy.set_index("Date")
        df_ccy = df_ccy[["Close"]].dropna()

        scaler = MinMaxScaler()
        scaled = scaler.fit_transform(df_ccy)

        def create_sequences(data, window):
            X, y = [], []
            for i in range(len(data) - window):
                X.append(data[i:i+window])
                y.append(data[i+window])
            return np.array(X), np.array(y)

        WINDOW = 30
        X, y = create_sequences(scaled, WINDOW)
        X_rf = X.reshape(X.shape[0], -1)
        X_lstm = X.reshape(X.shape[0], X.shape[1], 1)
        split = int(len(X) * 0.8)

        X_train_rf, X_test_rf = X_rf[:split], X_rf[split:]
        X_train_lstm, X_test_lstm = X_lstm[:split], X_lstm[split:]
        y_train, y_test = y[:split], y[split:]

        results = {}

        # Random Forest with Grid Search
        rf_params = {"n_estimators": [50, 100], "max_depth": [5, 10, None]}
        rf_grid = GridSearchCV(RandomForestRegressor(random_state=42), rf_params, cv=3, n_jobs=-1)
        rf_grid.fit(X_train_rf, y_train.ravel())
        best_rf = rf_grid.best_estimator_
        results["Random Forest"] = best_rf.predict(X_test_rf)

        # LSTM
        lstm = Sequential()
        lstm.add(LSTM(64, return_sequences=False, input_shape=(WINDOW, 1)))
        lstm.add(Dropout(0.2))
        lstm.add(Dense(1))
        lstm.compile(optimizer="adam", loss="mse")
        lstm.fit(X_train_lstm, y_train, epochs=30, batch_size=16, verbose=0)
        results["LSTM"] = lstm.predict(X_test_lstm).ravel()

        # Linear Regression (no hyperparameter tuning needed)
        lr = LinearRegression()
        lr.fit(X_train_rf, y_train)
        results["Linear Regression"] = lr.predict(X_test_rf)

        # Ridge Regression with Grid Search
        ridge_params = {"alpha": [0.1, 1.0, 10.0]}
        ridge_grid = GridSearchCV(Ridge(), ridge_params, cv=3)
        ridge_grid.fit(X_train_rf, y_train.ravel())
        results["Ridge"] = ridge_grid.best_estimator_.predict(X_test_rf)

        # KNN with Grid Search
        knn_params = {"n_neighbors": [3, 5, 7]}
        knn_grid = GridSearchCV(KNeighborsRegressor(), knn_params, cv=3)
        knn_grid.fit(X_train_rf, y_train.ravel())
        results["KNN"] = knn_grid.best_estimator_.predict(X_test_rf)

        # SVR with Grid Search
        svr_params = {"C": [0.1, 1.0, 10.0], "kernel": ["rbf", "linear"]}
        svr_grid = GridSearchCV(SVR(), svr_params, cv=3)
        svr_grid.fit(X_train_rf, y_train.ravel())
        results["SVR"] = svr_grid.best_estimator_.predict(X_test_rf)

        y_true = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
        df_metrics = []

        fig = go.Figure()
        fig.add_trace(go.Scatter(x=df_ccy.index[-len(y_true):], y=y_true, name="Actual", line=dict(color="white")))

        for model, preds in results.items():
            preds_inv = scaler.inverse_transform(preds.reshape(-1, 1)).flatten()
            fig.add_trace(go.Scatter(x=df_ccy.index[-len(y_true):], y=preds_inv, name=model))
            df_metrics.append({
                "Model": model,
                "MAPE": mean_absolute_percentage_error(y_true, preds_inv) * 100,
                "MAE": mean_absolute_error(y_true, preds_inv),
                "RMSE": np.sqrt(mean_squared_error(y_true, preds_inv))
            })

        fig.update_layout(title=f"{selected_currency} Forecast Comparison", xaxis_title="Date", yaxis_title="FX Rate")
        st.plotly_chart(fig, use_container_width=True)
        st.subheader(f"Manual Threshold:{thresholds["ManualThreshold"].values[0]}")
        st.markdown(f"""
        **Threshold Interpretation Example (for {selected_currency}):**
        - If a deal deviates more than **MAPE%** from forecasted market rate, flag as **off-market**.
        - Deviation (%) = abs(|Deal - Market| / Market Ã— 100)
        """)
        st.dataframe(pd.DataFrame(df_metrics,columns=["Model","MAPE"]), use_container_width=True)
        # --- Feedback Loop: Store metrics to CSV ---
        log_path = "model_feedback_log.csv"
        log_data = []
        for row in df_metrics:
            log_data.append({
                "RunDate": pd.Timestamp.now().date(),
                "Currency": selected_currency,
                "Model": row["Model"],
                "MAPE": round(row["MAPE"], 4),
                "MAE": round(row["MAE"], 4),
                "RMSE": round(row["RMSE"], 4),
                "Dataset": suffix.replace("(", "").replace(")", "")
            })

        log_df = pd.DataFrame(log_data)

        if os.path.exists(log_path):
            old_log = pd.read_csv(log_path)
            full_log = pd.concat([old_log, log_df], ignore_index=True)
            full_log = full_log.drop_duplicates()
        else:
            full_log = log_df

        full_log.to_csv(log_path, index=False)


    with tab4_sub1:
        currencies = df["Currency"].unique().tolist()
        suffix="(Original)"
        selected_currency = st.selectbox(f"Select Currency {suffix}", currencies,index=None, key=f"ccy{suffix}")
        if selected_currency:          
            run_forecast(df,suffix)
        else:
            st.warning("Select Currency to Forecast Trend using model and calibarate the threshold")    

    with tab4_sub2:
        test_df = pd.read_csv("reuters_gfx_test_data.csv")
        test_df["Date"] = pd.to_datetime(test_df["Date"])
        currencies = df["Currency"].unique().tolist()
        suffix="(Original_+_Shocked)"
        selected_currency = st.selectbox(f"Select Currency {suffix}", currencies,index=None, key=f"ccy{suffix}")  
        if selected_currency:                    
            run_forecast(test_df.rename(columns={"Close_Shocked": "Close"}),suffix)
        else:
            st.warning("Select Currency to Forecast Trend using model and calibarate the threshold")   

    with tab4_sub3:
        # --- Feedback Loop Viewer ---
        with st.expander(" View Historical Model Feedback Summary"):
            log_path = "model_feedback_log.csv"
            if os.path.exists(log_path):
                feedback_df = pd.read_csv(log_path)
                st.dataframe(feedback_df.sort_values("RunDate", ascending=False), use_container_width=True)
                csv = feedback_df.to_csv(index=False,mode='a').encode("utf-8")
                st.download_button(" Download Feedback CSV", data=csv, file_name="model_feedback_log.csv", mime="text/csv",key="Download Feedloop Download Feedback CSV(Shocked) ")
            else:
                st.warning("No feedback history found yet. Run a forecast to populate this log.")  
