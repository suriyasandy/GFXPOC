# --- PART 1: Imports, Global Configs, and Utility Caching ---
import streamlit as st
import pandas as pd
import numpy as np
import os, joblib
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
from arch import arch_model
from scipy.stats import genpareto, kurtosis, skew
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error,root_mean_squared_error,mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
import keras
from datetime import datetime
from keras.layers import Conv1D, MaxPooling1D, Flatten
from keras.models import Sequential
from keras.layers import LSTM, Dropout, Dense
from keras.callbacks import EarlyStopping
from keras.models import load_model
from prophet import Prophet
from statsmodels.tsa.stattools import adfuller
import statsmodels.api as sm
from xgboost import XGBRegressor
import shap  

ROLL_WINDOW = 60
ANNUALIZE = np.sqrt(252)
PCT_THRESHOLD = 0.95
EVT_TAIL_PCT = 0.995

# Page config
st.set_page_config(page_title="FX Volatility App", layout="wide")
ROLL_WINDOW = st.sidebar.slider("Rolling Window (Days)", 20, 120, 60,step=10)
# Background CSS
st.markdown("""
<style>
.stApp { background-color: light-gray; }
</style>
""", unsafe_allow_html=True)

# --- FX Data Loader ---
@st.cache_data
def load_fx_data(roll_window):
    df = pd.read_csv("reuters_fx_data.csv")
    df["Date"] = pd.to_datetime(df["Date"])
    df.sort_values(["Currency", "Date"], inplace=True)
    df["LogReturn"] = df.groupby("Currency")["Close"].transform(lambda x: np.log(x).diff())
    df["Volatility"] = df.groupby("Currency")["LogReturn"].transform(lambda x: x.rolling(roll_window).std()) * ANNUALIZE
    df["RealizedVol"] = df["OHLCVolatility"] / np.sqrt(252)
    return df.dropna()

# --- Sequence Creator for ML Models ---
def create_sequences(data, window):
    X, y = [], []
    if isinstance(data, pd.Series):
        data = data.dropna().values
    else:
        data = data[~np.isnan(data).any(axis=1)]
    for i in range(len(data) - window):
        X.append(data[i:i+window])
        y.append(data[i+window])
    return np.array(X), np.array(y)


# --- Model Loader ---
def load_models(currency_name, path="models"):
    try:
        rf = joblib.load(f"{path}/rf_{currency_name}.pkl")
        svr = joblib.load(f"{path}/svr_{currency_name}.pkl")
        lr = joblib.load(f"{path}/lr_{currency_name}.pkl")
        lstm = load_model(f"{path}/lstm_{currency_name}.h5", compile=False)
        scaler = joblib.load(f"{path}/scaler_{currency_name}.pkl")
        return {"Random Forest": rf, "SVR": svr, "Linear Regression": lr, "LSTM": lstm}, scaler
    except Exception as e:
        st.warning(f"Model loading failed for {currency_name}: {e}")
        return None, None

# --- Model Training and Saving ---
def train_and_save_models(currency_name, df, save_path="models", window=ROLL_WINDOW):
    os.makedirs(save_path, exist_ok=True)
    df_currency = df[df["Currency"] == currency_name][["Date", "Close"]].dropna().copy()
    df_currency["Date"] = pd.to_datetime(df_currency["Date"])
    df_currency.set_index("Date", inplace=True)

    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(df_currency)

    X, y = create_sequences(scaled_data, window)
    X_rf = X.reshape(X.shape[0], -1)
    X_lstm = X.reshape(X.shape[0], X.shape[1], 1)
    y = y.ravel()

    rf_params = {"n_estimators": [50, 100], "max_depth": [5, 10, None]}
    rf = GridSearchCV(RandomForestRegressor(random_state=42), rf_params, cv=TimeSeriesSplit(n_splits=3))
    rf.fit(X_rf, y)
    joblib.dump(rf.best_estimator_, f"{save_path}/rf_{currency_name}.pkl")

    svr_params = {"C": [1, 10], "gamma": ["scale", "auto"], "epsilon": [0.01, 0.1]}
    svr = GridSearchCV(SVR(), svr_params, cv=TimeSeriesSplit(n_splits=3))
    svr.fit(X_rf, y)
    joblib.dump(svr.best_estimator_, f"{save_path}/svr_{currency_name}.pkl")

    lr = LinearRegression()
    lr.fit(X_rf, y)
    joblib.dump(lr, f"{save_path}/lr_{currency_name}.pkl")

    lstm = Sequential()
    lstm.add(LSTM(64, input_shape=(X_lstm.shape[1], X_lstm.shape[2])))
    lstm.add(Dropout(0.2))
    lstm.add(Dense(1))
    lstm.compile(loss='mse', optimizer='adam')

    early_stop = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)
    lstm.fit(X_lstm, y, epochs=20, batch_size=16, verbose=0, callbacks=[early_stop])
    lstm.save(f"{save_path}/lstm_{currency_name}.h5")

    joblib.dump(scaler, f"{save_path}/scaler_{currency_name}.pkl")
    return True


# --- FX Dataset ---
df = load_fx_data(ROLL_WINDOW)


# Sidebar: Controls
# Sidebar: Controls
st.sidebar.header("⚙️ Train FX Forecasting Models")

# Currency selection
currency_option = st.sidebar.selectbox("Select Currency", df["Currency"].unique())

# Model path input
model_dir = st.sidebar.text_input("Model Save Path", value="models")

# Force retrain checkbox
force_retrain = st.sidebar.checkbox("Force Retrain Models", value=False)

# Model training trigger
if st.sidebar.button("Train & Save Models"):
    model_path_check = all([
        os.path.exists(f"{model_dir}/rf_{currency_option}.pkl"),
        os.path.exists(f"{model_dir}/svr_{currency_option}.pkl"),
        os.path.exists(f"{model_dir}/lr_{currency_option}.pkl"),
        os.path.exists(f"{model_dir}/lstm_{currency_option}.h5")
    ])
    if not model_path_check or force_retrain:
        with st.spinner(f"Training models for {currency_option}..."):
            success = train_and_save_models(currency_option, df, save_path=model_dir, window=ROLL_WINDOW)
        if success:
            st.sidebar.success(f" Models for `{currency_option}` saved to `{model_dir}`")
            st.rerun()
    else:
        st.sidebar.info(" Models already exist. Use 'Force Retrain' to overwrite.")


# View log comparison
if st.sidebar.button(" View Model Feedback Log"):
    try:
        log_df = pd.read_csv("logs/model_feedback_log.csv")
        st.sidebar.dataframe(log_df[log_df["Currency"] == currency_option])
    except Exception as e:
        st.sidebar.error("No log found yet.")




# Assign manual volatility group
def assign_manual_group(vol):
    if vol < 0.07:
        return "Group 1"
    elif vol < 0.50:
        return "Group 2"
    elif vol < 0.60:
        return "Group 3"
    else:
        return "Group 4"


@st.cache_data
def compute_thresholds_per_currency(df,roll_window):
    summary = []
    for ccy, group in df.groupby("Currency"):
        log_ret = group["LogReturn"].dropna()
        # Volatility for dynamic models (unchanged)
        vol_series = group["LogReturn"].rolling(roll_window).std() * ANNUALIZE
        avg_vol = vol_series.mean()

        # Manual group uses realized OHLC volatility
        realized_series = group["RealizedVol"].dropna()
        avg_realized_vol = realized_series.mean()

        # Manual group and threshold (based on OHLC-based logic)
        manual_group = assign_manual_group(avg_realized_vol)
        manual_threshold = {
            "Group 1": 0.10,
            "Group 2": 0.25,
            "Group 3": 0.55,
            "Group 4": 0.80
        }[manual_group]

        try:
            am = arch_model(vol_series, vol='GARCH', p=1, q=1)
            res = am.fit(disp="off")
            forecast = res.forecast(horizon=1).variance.values[-1][0] ** 0.5
        except:
            forecast = np.nan

        tail_data = vol_series[vol_series > vol_series.quantile(EVT_TAIL_PCT)]
        try:
            evt_params = genpareto.fit(tail_data)
            evt_threshold = genpareto.ppf(0.999, *evt_params)
        except:
            evt_threshold = np.nan

        # GARCH forecast should use raw log returns
        try:
            am = arch_model(log_ret, vol='GARCH', p=1, q=1)
            res = am.fit(disp="off")
            forecast = res.forecast(horizon=1).variance.values[-1][0] ** 0.5
        except:
            forecast = np.nan
        summary.append({
            "Currency": ccy,
            "AvgVol": avg_vol,
            "AvgRealizedVol": avg_realized_vol,
            "ManualGroup": manual_group,
            "ManualThreshold": manual_threshold,
            "GARCH_Forecast": forecast,
            "95th_Pct": vol_series.quantile(PCT_THRESHOLD),
            "EVT_Threshold": evt_threshold
        })
    return pd.DataFrame(summary)



df_summary = compute_thresholds_per_currency(df,ROLL_WINDOW)


# Tab layout
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "Overview", "Thresholds", "Live FX Rate Forecasting",
    "Final Results & Recommendation", "Cross-Currency View"
])

with tab1:
    st.header(" FX Volatility Monitoring: Manual vs Dynamic")

    st.markdown("""
    This app helps identify **abnormal FX behavior** by comparing:

    -  **Manual Thresholds**: Fixed bands, used operationally today.
    -  **Dynamic Thresholds**: Adjust to real market risk using GARCH, EVT, and percentiles.
    -  **Forecast Models**: Predict upcoming risk using AI/ML + time series methods.

    ---
    ###  Manual Threshold Issues
    - Static bands don't reflect current risk.
    - Cannot handle shocks (e.g. elections, inflation prints).
    - Trigger false alerts or miss critical spikes.

    ---
    ###  Dynamic Threshold Benefits
    - Adapted from statistical/ML models using rolling vol.
    - Explainable and validated per currency.
    - Improve alert accuracy and stakeholder trust.
                
    ###  What is the Rolling Window?

    A rolling window defines how many past days are used to compute volatility:
    - Smaller windows (e.g., 30 days) → More responsive but noisier thresholds
    - Larger windows (e.g., 90+ days) → Smoother trends but slower to detect shocks

    You can choose this window in the sidebar.  
    All dynamic models (GARCH, EVT, synthetic volatility, and forecasts) will use this setting for consistency.

    ### Why are dynamic volatilities often higher than manual ones?**  
    - Manual thresholds rely on smoothed OHLC-based realized volatility, 
    - While dynamic thresholds use log-return-based volatility — which captures sudden spikes and real-time market stress more effectively.

    Use the tabs to explore per-currency insights.
    """)

with tab2:
    st.header(" Compare Thresholds: Manual vs Dynamic vs Technical")

    st.markdown("""
    This tab compares:
    -  Static **manual thresholds** from OHLC rules
    -  Dynamic models (GARCH, EVT, percentile)
    -  Technical thresholds (ATR, SuperTrend, Donchian)

    Stakeholders can see where manual logic fails to capture volatility shifts.
    """)
    selected_currency = st.selectbox("Select Currency", df["Currency"].unique(),index=None, key="tab1_currency")
    if selected_currency:
        df_currency = df[df["Currency"] == selected_currency].copy().sort_values("Date")
        df_summary_sel = df_summary[df_summary["Currency"] == selected_currency]
        st.dataframe(df_summary_sel[[ "Currency", "AvgVol", "AvgRealizedVol",
                                "ManualGroup", "ManualThreshold", 
                                "GARCH_Forecast", "95th_Pct", "EVT_Threshold"]],
                    use_container_width=True)
        series = df[df["Currency"] == selected_currency].copy().sort_values("Date")
        series["LogReturn"] = np.log(series["Close"]).diff()

        tr1 = series["High"] - series["Low"]
        tr2 = (series["High"] - series["Close"].shift()).abs()
        tr3 = (series["Low"] - series["Close"].shift()).abs()
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        series["ATR"] = tr.rolling(window=14).mean()
        series["ATR_Threshold"] = series["ATR"] * 1.5
        series["DonchianUpper"] = series["High"].rolling(20).max()
        series["DonchianLower"] = series["Low"].rolling(20).min()
        hl2 = (series["High"] + series["Low"]) / 2
        series["SuperTrend"] = np.where(series["Close"] > hl2 - (2 * series["ATR"]),
                                        hl2 - (2 * series["ATR"]), hl2 + (2 * series["ATR"]))
        # --- (A) Volatility Threshold Comparison ---
        fig = px.line(series, x="Date", y="OHLCVolatility", title=f"{selected_currency}: Volatility vs Thresholds")
        for label in ["ManualThreshold", "GARCH_Forecast", "95th_Pct", "EVT_Threshold"]:
            val = df_summary[df_summary["Currency"] == selected_currency][label].values[0]
            if pd.notnull(val):
                fig.add_hline(y=val, line_dash="dot", annotation_text=label)
        st.plotly_chart(fig, use_container_width=True)

        # --- (B) Technical Indicator Plot ---
        with st.expander("📊 View Technical Indicators on Price"):
            fig2 = go.Figure()
            fig2.add_trace(go.Scatter(x=series["Date"], y=series["Close"], name="Close Price"))
            fig2.add_trace(go.Scatter(x=series["Date"], y=series["SuperTrend"], name="SuperTrend", line=dict(dash="dot")))
            fig2.add_trace(go.Scatter(x=series["Date"], y=series["DonchianUpper"], name="Donchian Upper", line=dict(dash="dot")))
            fig2.add_trace(go.Scatter(x=series["Date"], y=series["ATR_Threshold"], name="ATR Threshold", line=dict(dash="dot")))

            fig2.update_layout(title=f"{selected_currency}: Price vs Technical Indicators", xaxis_title="Date", yaxis_title="Price")
            st.plotly_chart(fig2, use_container_width=True)

            st.dataframe(series[["Date", "Close", "ATR_Threshold", "DonchianUpper", "SuperTrend"]].tail(30), use_container_width=True)

        st.markdown("""
        ###  Understanding the Threshold Models

        Each dynamic threshold serves a distinct purpose. Here's how to interpret them:

        | Threshold Type       | What it Measures                                | Typical Level | Purpose                      |
        |----------------------|--------------------------------------------------|---------------|-------------------------------|
        | **GARCH Forecast**   | Expected near-term volatility (next 1 day)       | 🟡 Lower       | Stable market forecasting     |
        | **95th Percentile**  | Top 5% of historical rolling volatilities        | 🟠 High        | Detect sharp volatility moves |
        | **EVT Threshold**    | Statistically rare tail events                   | 🔴 Very High   | Capture extreme regime shifts |
        | **Manual Threshold** | Fixed, rule-based band from OHLC behavior        | ⚪️ Static      | Operational baseline           |

        > GARCH is best for short-term expectations,  
        > while Percentile and EVT thresholds act as **market stress detectors**.

        By combining all three, we can **confidently flag real risk** without triggering false alerts.
        """)
        with st.expander("Explore Close Pricce, KDE, Boxplot, and Seasonality"):
            st.subheader(f" {selected_currency} Close Price Trend")
            fig = px.line(df_currency, x="Date", y="Close")
            st.plotly_chart(fig, use_container_width=True)            
            fig_kde = px.histogram(df_currency, x='Close', histnorm='probability density')
            st.plotly_chart(fig_kde, use_container_width=True)

            fig_box = px.box(df_currency.melt(value_vars=["Open", "High", "Low", "Close"]),
                            x="variable", y="value", title="OHLC Distribution")
            st.plotly_chart(fig_box, use_container_width=True)

            try:
                decomposition = sm.tsa.seasonal_decompose(df_currency.set_index("Date")["Close"], model="additive", period=30)
                fig_decomp = go.Figure()
                fig_decomp.add_trace(go.Scatter(x=decomposition.trend.index, y=decomposition.trend, name="Trend"))
                fig_decomp.add_trace(go.Scatter(x=decomposition.seasonal.index, y=decomposition.seasonal, name="Seasonal"))
                fig_decomp.add_trace(go.Scatter(x=decomposition.resid.index, y=decomposition.resid, name="Residual"))
                fig_decomp.update_layout(title="Seasonal Decomposition")
                st.plotly_chart(fig_decomp, use_container_width=True)
            except Exception as e:
                st.warning(f"Decomposition error: {e}")
        with st.expander(" Last 30-Day Threshold Snapshot"):
            st.dataframe(series[[ "Date", "OHLCVolatility", "ATR_Threshold",
                                "DonchianUpper", "SuperTrend" ]].tail(30), use_container_width=True)
# --- Tab 3: Forecast with Dynamic Test Period ---
with tab3:
    st.header("Live FX Forecasting (Flexible Test Period)")

    selected_currency = st.selectbox("Select Currency", df["Currency"].unique(),index=None, key="fx_forecast_currency")
    test_days = st.slider("Select Test Period (days before latest date)", min_value=30, max_value=90, step=30, value=60)

    if selected_currency:
        df_ccy = df[df["Currency"] == selected_currency][["Date", "Close"]].dropna().copy()
        df_ccy["Date"] = pd.to_datetime(df_ccy["Date"])
        df_ccy.set_index("Date", inplace=True)

        latest_date = df_ccy.index.max()
        test_start = latest_date - pd.Timedelta(days=test_days)
        train_df = df_ccy[df_ccy.index < test_start]
        test_df = df_ccy[df_ccy.index >= test_start]

        if train_df.empty or test_df.empty:
            st.error("Not enough data for selected test period.")
        else:
            scaler = MinMaxScaler()
            scaled_all = scaler.fit_transform(df_ccy)
            scaled_train = scaler.transform(train_df)
            scaled_test = scaler.transform(test_df)

            def create_sequences(data, window):
                X, y = [], []
                for i in range(len(data) - window):
                    X.append(data[i:i+window])
                    y.append(data[i+window])
                return np.array(X), np.array(y)

            WINDOW = 30
            X_train, y_train = create_sequences(scaled_train, WINDOW)
            X_rf_test = []
            for i in range(len(test_df)):
                X_rf_test.append(scaled_all[-(len(test_df) + WINDOW - i):-len(test_df) + i])
            X_rf_test = np.array(X_rf_test).reshape(len(X_rf_test), -1)
            X_train_rf = X_train.reshape(X_train.shape[0], -1)
            X_train_lstm = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_lstm_test = np.array([x.reshape(WINDOW, 1) for x in X_rf_test])

            model_outputs_train, model_outputs_test = {}, {}
            rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
            rf.fit(X_train_rf, y_train.ravel())
            model_outputs_train["Random Forest"] = rf.predict(X_train_rf)
            model_outputs_test["Random Forest"] = rf.predict(X_rf_test)

            lstm = Sequential()
            lstm.add(LSTM(64, input_shape=(WINDOW, 1)))
            lstm.add(Dropout(0.2))
            lstm.add(Dense(1))
            lstm.compile(optimizer="adam", loss="mse")
            lstm.fit(X_train_lstm, y_train, epochs=20, batch_size=16, verbose=0)
            model_outputs_train["LSTM"] = lstm.predict(X_train_lstm).ravel()
            model_outputs_test["LSTM"] = lstm.predict(X_lstm_test).ravel()

            lr = LinearRegression()
            lr.fit(X_train_rf, y_train)
            model_outputs_train["Linear Regression"] = lr.predict(X_train_rf)
            model_outputs_test["Linear Regression"] = lr.predict(X_rf_test)

            y_true_train = scaler.inverse_transform(y_train.reshape(-1, 1)).flatten()
            y_true_test = test_df["Close"].values

            fig = go.Figure()
            fig.add_trace(go.Scatter(x=train_df.index[-len(y_true_train):], y=y_true_train, name="Actual (Train)", line=dict(color="white")))
            fig.add_trace(go.Scatter(x=test_df.index, y=y_true_test, name="Actual (Test)", line=dict(color="white", dash="dot")))

            metrics = []
            for model, preds_train in model_outputs_train.items():
                preds_train_inv = scaler.inverse_transform(preds_train.reshape(-1, 1)).flatten()
                preds_test_inv = scaler.inverse_transform(model_outputs_test[model].reshape(-1, 1)).flatten()

                fig.add_trace(go.Scatter(x=train_df.index[-len(y_true_train):], y=preds_train_inv, name=f"{model} (Train)", line=dict(dash="dash")))
                fig.add_trace(go.Scatter(x=test_df.index, y=preds_test_inv, name=f"{model} (Test)", line=dict(dash="solid")))

                metrics.append({
                    "Model": model,
                    "Train MAPE": mean_absolute_percentage_error(y_true_train, preds_train_inv) * 100,
                    "Test MAPE": mean_absolute_percentage_error(y_true_test, preds_test_inv) * 100,
                    "Train RMSE": np.sqrt(mean_squared_error(y_true_train, preds_train_inv)),
                    "Test RMSE": np.sqrt(mean_squared_error(y_true_test, preds_test_inv))
                })

            best_model = min(metrics, key=lambda x: x["Test MAPE"])
            model_name = best_model["Model"]
            best_mape = best_model["Test MAPE"]
            best_rmse = best_model["Test RMSE"]

            log_entry = pd.DataFrame([{
                "RunDate": pd.Timestamp.today(),
                "Currency": selected_currency.upper(),  # <- ensure uppercase
                "Model": model_name,
                "MAPE": best_mape,
                "RMSE": best_rmse
            }])
            log_dir = "logs"
            os.makedirs(log_dir, exist_ok=True)
            log_path = f"{log_dir}/model_feedback_log.csv"
            if os.path.exists(log_path):
                existing = pd.read_csv(log_path)
                full_log = pd.concat([existing, log_entry], ignore_index=True)
            else:
                full_log = log_entry
            full_log.to_csv(log_path, index=False)
            #st.write("DEBUG: Log Data for INR", full_log[full_log["Currency"] == selected_currency])

            preds_inv = scaler.inverse_transform(model_outputs_test[model_name].reshape(-1, 1)).flatten()
            mape_pct = best_mape / 100
            upper_bound = preds_inv * (1 + mape_pct)
            lower_bound = preds_inv * (1 - mape_pct)
            flag_mask = (y_true_test > upper_bound) | (y_true_test < lower_bound)

            fig.add_trace(go.Scatter(x=test_df.index, y=upper_bound, name="Dynamic Upper", line=dict(dash="dot", color="green")))
            fig.add_trace(go.Scatter(x=test_df.index, y=lower_bound, name="Dynamic Lower", line=dict(dash="dot", color="green")))
            fig.add_trace(go.Scatter(
                x=test_df.index[flag_mask],
                y=y_true_test[flag_mask],
                mode="markers",
                name="Flagged Deviations",
                marker=dict(color="red", size=10, symbol="x")
            ))

            fig.update_layout(
                title=f"{selected_currency}: Forecast vs Thresholds ({model_name})",
                xaxis_title="Date",
                yaxis_title="FX Rate"
            )
            st.plotly_chart(fig, use_container_width=True)


# --- Tab 4: Final Thresholds & Forecast Accuracy ---
with tab4:
    st.header("Final Thresholds & Forecast Accuracy")

    df_summary["FinalThreshold"] = df_summary[[
         "95th_Pct", "GARCH_Forecast", "EVT_Threshold"
    ]].max(axis=1)

    @st.cache_data
    def load_best_mape_summary():
        try:
            log_df = pd.read_csv("logs/model_feedback_log.csv")
            log_df["Currency"] = log_df["Currency"].str.strip().str.upper()
            df_summary["Currency"] = df_summary["Currency"].str.strip().str.upper()
            latest = log_df.sort_values("RunDate", ascending=False).drop_duplicates("Currency")
            return latest[["Currency", "Model", "MAPE", "RMSE"]].rename(
                columns={"MAPE": "Best_MAPE", "RMSE": "Best_RMSE"})
        except Exception as e:
            st.error(f"Error loading model summary: {e}")
            return pd.DataFrame(columns=["Currency", "Best_MAPE", "Model", "Best_RMSE"])



    selected_currency = st.selectbox("Select Currency", df["Currency"].unique(),index=None,key="tab4_currency")
    if selected_currency:
        best_mape_df = load_best_mape_summary()
        df_final = df_summary.merge(best_mape_df, on="Currency", how="left", validate="one_to_one")
        selected_currency = selected_currency.upper()
        row = df_final[df_final["Currency"] == selected_currency].iloc[0]

        st.subheader(f"{selected_currency}: Thresholds & Justification")
        st.markdown(f"""
        - **Avg Log-Return Volatility** (Dynamic): `{round(row['AvgVol'], 4)}`  
        - **Avg OHLC Realized Volatility** (Manual): `{round(row['AvgRealizedVol'], 4)}`  
        - **Manual Threshold**: `{round(row['ManualThreshold'], 4)}`  
        - **Final Dynamic Threshold**: `{round(row['FinalThreshold'], 4)}`  
        - **Best Forecasting Model**: `{row['Model']}`  
        - **MAPE**: `{round(row['Best_MAPE'], 2)}%`
        """)

        # Load volatility and scale
        trend_df = df[df["Currency"] == selected_currency].copy().sort_values("Date")
        y_actual = trend_df["Volatility"].values
        dates = trend_df["Date"]

        # MAPE Soft Bands
        mape_pct = row["Best_MAPE"] / 100 if pd.notnull(row["Best_MAPE"]) else 0.05
        upper_band = y_actual * (1 + mape_pct)
        lower_band = y_actual * (1 - mape_pct)
        flag_mask = (y_actual > upper_band) | (y_actual < lower_band)

        # Confidence Interval from residuals
        model_dict, _ = load_models(selected_currency)
        ci_upper, ci_lower, ci_dates = None, None, None
        if model_dict and row["Model"] in model_dict:
            try:
                # Use Close price as proxy since Volatility is not target in training
                series = trend_df["Close"].values.reshape(-1, 1)
                scaled = MinMaxScaler().fit_transform(series)
                X_seq, y_seq = create_sequences(scaled, window=ROLL_WINDOW)

                if row["Model"] == "LSTM":
                    preds = model_dict["LSTM"].predict(X_seq).ravel()
                else:
                    preds = model_dict[row["Model"]].predict(X_seq.reshape(X_seq.shape[0], -1))
                residuals = y_seq - preds
                resid_std = np.std(residuals)
                ci_upper = preds + 1.96 * resid_std
                ci_lower = preds - 1.96 * resid_std
                ci_dates = trend_df["Date"].iloc[-len(preds):]
            except Exception as e:
                st.warning("Confidence interval error: " + str(e))

        # Plot
        fig_vol = go.Figure()
        fig_vol.add_trace(go.Scatter(x=dates, y=y_actual, name="Actual Volatility", line=dict(color="white")))
        fig_vol.add_trace(go.Scatter(x=dates, y=upper_band, name="MAPE Upper Band", line=dict(dash="dot", color="green")))
        fig_vol.add_trace(go.Scatter(x=dates, y=lower_band, name="MAPE Lower Band", line=dict(dash="dot", color="green")))

        if ci_upper is not None:
            fig_vol.add_trace(go.Scatter(x=ci_dates, y=ci_upper, name="Stat Upper CI", line=dict(dash="dash", color="cyan")))
            fig_vol.add_trace(go.Scatter(x=ci_dates, y=ci_lower, name="Stat Lower CI", line=dict(dash="dash", color="cyan")))

        fig_vol.add_hline(y=row["ManualThreshold"], line_color="red", line_dash="dot", annotation_text="Manual")
        fig_vol.add_hline(y=row["FinalThreshold"], line_color="orange", line_dash="dash", annotation_text="Final Dynamic")

        fig_vol.add_trace(go.Scatter(
            x=dates[flag_mask], y=y_actual[flag_mask],
            mode="markers", name="Flagged Deviations",
            marker=dict(color="red", size=8, symbol="x")
        ))

        fig_vol.update_layout(
            title=f"{selected_currency}: Volatility vs Thresholds & Bands",
            xaxis_title="Date", yaxis_title="Annualized Volatility"
        )
        st.plotly_chart(fig_vol, use_container_width=True)

        st.markdown(f"""
        ### 📌 Interpretation Summary

        #### 🔴 Manual Threshold
        - Based on **OHLC Realized Volatility** (slow to react)
        - Realized Vol = `{row['AvgRealizedVol']:.4f}`  
        - Manual Threshold = `{row['ManualThreshold']:.4f}`

        #### 🟠 Final Dynamic Threshold (Hard Limit)
        - Based on GARCH, EVT, and 95th percentile
        - Final = `{row['FinalThreshold']:.4f}`  
        → **{"Higher" if row['FinalThreshold'] > row['ManualThreshold'] else "Lower"}** than manual → reflects real-time risk

        #### 🟢 MAPE Bands (Soft)
        - Based on model forecast error range: ±{round(mape_pct*100)}%
        - Volatility × (1 ± MAPE)
        - Helps identify deviations earlier

        #### 🔵 Residual Confidence Interval
        - Derived from actual model prediction error
        - CI = forecast ± 1.96 × std(residuals)
        - More robust, can guide confidence in alerts

        ---
        ✅ Use **Final Dynamic** for official breach thresholds  
        ✅ Use **MAPE** & **Residual CI** for early alerts & model trust
        """)


with tab5:
    st.header("Cross-Currency Thresholds (Synthetic FX)")

    st.markdown("""
    Synthetic FX pairs are constructed by combining two USD-based currencies:  
    `Base / Quote = (BaseUSD) / (QuoteUSD)`  

    We compare 3 key thresholds:  
    -  Manual Threshold (from OHLC avg volatility)  
    -  Dynamic Threshold (95th percentile of synthetic volatility)  
    -  Model Confidence Bands (statistical residuals from linear model)
    """)

    currencies = df["Currency"].unique().tolist()
    cross_pairs = [(f"{b}{q}", b, q) for b in currencies for q in currencies if b != q]
    pair_labels = [f"{p[0]} = {p[1]}USD ÷ {p[2]}USD" for p in cross_pairs]
    pair_choice = st.selectbox("Select Cross Pair", pair_labels, index=None)

    if pair_choice:
        pair_name, base_ccy, quote_ccy = cross_pairs[pair_labels.index(pair_choice)]

        try:
            base_df = df[df["Currency"] == base_ccy][["Date", "Close"]].copy()
            quote_df = df[df["Currency"] == quote_ccy][["Date", "Close"]].copy()

            merged = pd.merge(base_df, quote_df, on="Date", suffixes=(f"_{base_ccy}", f"_{quote_ccy}")).dropna()
            merged["SyntheticRate"] = merged[f"Close_{base_ccy}"] / merged[f"Close_{quote_ccy}"]
            merged["LogReturn"] = np.log(merged["SyntheticRate"]).diff()
            merged["CrossVolatility"] = merged["LogReturn"].rolling(ROLL_WINDOW).std() * ANNUALIZE

            # Manual-style volatility via synthetic OHLC
            merged["Synthetic_High"] = merged[[f"Close_{base_ccy}", f"Close_{quote_ccy}"]].max(axis=1)
            merged["Synthetic_Low"] = merged[[f"Close_{base_ccy}", f"Close_{quote_ccy}"]].min(axis=1)
            merged["OHLCVolatility"] = (merged["Synthetic_High"] - merged["Synthetic_Low"]) / merged["SyntheticRate"]
            merged["ManualVolatility"] = merged["OHLCVolatility"].rolling(ROLL_WINDOW).mean() / np.sqrt(252)

            merged.dropna(inplace=True)

            df_thresh = df_summary.set_index("Currency")
            base_thresh = df_thresh.loc[base_ccy, "ManualThreshold"]
            quote_thresh = df_thresh.loc[quote_ccy, "ManualThreshold"]
            manual_cross = max(base_thresh, quote_thresh)
            dynamic_cross = merged["CrossVolatility"].quantile(PCT_THRESHOLD)

            merged["Breach_Manual"] = merged["CrossVolatility"] > manual_cross
            merged["Breach_Dynamic"] = merged["CrossVolatility"] > dynamic_cross

            # Residual-based CI (Linear Regression fit)
            from sklearn.linear_model import LinearRegression
            vol_series = merged["CrossVolatility"].values.reshape(-1, 1)
            X_seq, y_seq = create_sequences(vol_series, window=ROLL_WINDOW)
            model = LinearRegression().fit(X_seq.reshape(X_seq.shape[0], -1), y_seq)
            preds = model.predict(X_seq.reshape(X_seq.shape[0], -1))
            residuals = y_seq - preds
            resid_std = np.std(residuals)
            ci_upper = preds + 1.96 * resid_std
            ci_lower = preds - 1.96 * resid_std
            ci_dates = merged["Date"].iloc[-len(preds):]

            # Summary metrics
            manual_vol_avg = merged["ManualVolatility"].mean()
            dynamic_vol_avg = merged["CrossVolatility"].mean()

            st.subheader(f"🔍 {pair_name} Threshold & Volatility Summary")
            st.markdown(f"""
            - **Base `{base_ccy}` Manual Threshold**: `{base_thresh:.4f}`  
            - **Quote `{quote_ccy}` Manual Threshold**: `{quote_thresh:.4f}`  
            - **Manual Cross Threshold** (max of both): `{manual_cross:.4f}`  
            - **Dynamic Threshold** (95th percentile): `{dynamic_cross:.4f}`  
            - **Avg Realized (Manual)**: `{manual_vol_avg:.4f}`  
            - **Avg Log-Return (Dynamic)**: `{dynamic_vol_avg:.4f}`
            """)

            fig = go.Figure()
            fig.add_trace(go.Scatter(x=merged["Date"], y=merged["CrossVolatility"], name="Synthetic Volatility", line=dict(color="white")))
            fig.add_hline(y=manual_cross, line_color="red", line_dash="dot", annotation_text="Manual Threshold")
            fig.add_hline(y=dynamic_cross, line_color="orange", line_dash="dash", annotation_text="Dynamic Threshold")

            fig.add_trace(go.Scatter(x=ci_dates, y=ci_upper, name="Stat Upper CI", line=dict(dash="dot", color="cyan")))
            fig.add_trace(go.Scatter(x=ci_dates, y=ci_lower, name="Stat Lower CI", line=dict(dash="dot", color="cyan")))

            fig.add_trace(go.Scatter(
                x=merged[merged["Breach_Manual"]]["Date"],
                y=merged[merged["Breach_Manual"]]["CrossVolatility"],
                mode="markers", name="Manual Breach", marker=dict(color="red", symbol="circle", size=7)
            ))

            fig.add_trace(go.Scatter(
                x=merged[merged["Breach_Dynamic"]]["Date"],
                y=merged[merged["Breach_Dynamic"]]["CrossVolatility"],
                mode="markers", name="Dynamic Breach", marker=dict(color="orange", symbol="x", size=8)
            ))

            fig.update_layout(
                title=f"{pair_name}: Synthetic Volatility with Thresholds & Confidence Bands",
                xaxis_title="Date", yaxis_title="Annualized Volatility"
            )
            st.plotly_chart(fig, use_container_width=True)

            st.markdown("### 🧾 Interpretation Summary")
            st.markdown(f"""
            - 🔴 **Manual Threshold** is derived from OHLC-style volatility, slow to react.  
            - 🟠 **Dynamic Threshold** captures spikes in synthetic volatility.  
            - 🔵 **Confidence Intervals** built from statistical model offer trust-based monitoring zone.  
            - ❌ Markers show real breaches in manual/dynamic logic.

             Use **Dynamic Threshold** for hard breaches.  
             Use **Statistical Bands** as early warning for cross-pair deviations.
            """)

            st.markdown("### Last 30-Day Vol Snapshot")
            st.dataframe(merged[["Date", "SyntheticRate", "CrossVolatility", "ManualVolatility", "Breach_Manual", "Breach_Dynamic"]].tail(30), use_container_width=True)

        except Exception as e:
            st.error(f"Failed to compute cross-pair stats: {e}")
