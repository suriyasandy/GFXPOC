# --- PART 1: Imports, Global Configs, and Utility Caching ---
import streamlit as st
import pandas as pd
import numpy as np
import os, joblib
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
from arch import arch_model
from scipy.stats import genpareto, kurtosis, skew
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error,root_mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
import keras
from datetime import datetime
from keras.layers import Conv1D, MaxPooling1D, Flatten
from keras.models import Sequential
from keras.layers import LSTM, Dropout, Dense
from keras.callbacks import EarlyStopping
from keras.models import load_model
from prophet import Prophet
from statsmodels.tsa.stattools import adfuller
import statsmodels.api as sm
from xgboost import XGBRegressor
import shap  

ROLL_WINDOW = 60
ANNUALIZE = np.sqrt(252)
PCT_THRESHOLD = 0.95
EVT_TAIL_PCT = 0.995

# Page config
st.set_page_config(page_title="FX Volatility App", layout="wide")

# Background CSS
st.markdown("""
<style>
.stApp { background-color: light-gray; }
</style>
""", unsafe_allow_html=True)

# --- FX Data Loader ---
@st.cache_data
def load_fx_data():
    df = pd.read_csv("reuters_fx_data.csv")
    df["Date"] = pd.to_datetime(df["Date"])
    df.sort_values(["Currency", "Date"], inplace=True)
    df["LogReturn"] = df.groupby("Currency")["Close"].transform(lambda x: np.log(x).diff())
    df["Volatility"] = df.groupby("Currency")["LogReturn"].transform(lambda x: x.rolling(ROLL_WINDOW).std()) * ANNUALIZE
    df["RealizedVol"] = df["OHLCVolatility"] / np.sqrt(252)
    return df.dropna()

# --- Sequence Creator for ML Models ---
def create_sequences(data, window):
    X, y = [], []
    if isinstance(data, pd.Series):
        data = data.dropna().values
    else:
        data = data[~np.isnan(data).any(axis=1)]
    for i in range(len(data) - window):
        X.append(data[i:i+window])
        y.append(data[i+window])
    return np.array(X), np.array(y)

# --- Model Loader ---
def load_models(currency_name, path="models"):
    try:
        rf = joblib.load(f"{path}/rf_{currency_name}.pkl")
        svr = joblib.load(f"{path}/svr_{currency_name}.pkl")
        lr = joblib.load(f"{path}/lr_{currency_name}.pkl")
        lstm = load_model(f"{path}/lstm_{currency_name}.h5", compile=False)
        scaler = joblib.load(f"{path}/scaler_{currency_name}.pkl")
        return {"Random Forest": rf, "SVR": svr, "Linear Regression": lr, "LSTM": lstm}, scaler
    except Exception as e:
        st.warning(f"Model loading failed for {currency_name}: {e}")
        return None, None

# --- Model Training and Saving ---
def log_model_performance(currency, model_name, y_true, y_pred, log_file="model_feedback_log.csv"):
    metrics = {
        "Currency": currency,
        "Model": model_name,
        "MAE": mean_absolute_error(y_true, y_pred),
        "RMSE": root_mean_squared_error(y_true, y_pred),
        "MAPE": mean_absolute_percentage_error(y_true, y_pred),
        "Timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    log_entry = pd.DataFrame([metrics])
    if os.path.exists(log_file):
        existing = pd.read_csv(log_file)
        updated = pd.concat([existing, log_entry], ignore_index=True)
    else:
        updated = log_entry
    updated.to_csv(log_file, index=False)

def train_and_save_models(currency_name, df, save_path="models", window=30):
    os.makedirs(save_path, exist_ok=True)
    df_currency = df[df["Currency"] == currency_name][["Date", "Close"]].dropna().copy()
    df_currency["Date"] = pd.to_datetime(df_currency["Date"])
    df_currency.set_index("Date", inplace=True)

    # Normalize data
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(df_currency)

    # Create sequences
    X, y = create_sequences(scaled_data, window)
    X_rf = X.reshape(X.shape[0], -1)
    X_lstm = X.reshape(X.shape[0], X.shape[1], 1)
    y = y.ravel()

    # Train/Test split
    train_size = int(len(X_rf) * 0.8)
    X_train_rf, X_test_rf = X_rf[:train_size], X_rf[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    # --- Random Forest ---
    rf_params = {"n_estimators": [50, 100], "max_depth": [5, 10]}
    rf_grid = GridSearchCV(RandomForestRegressor(random_state=42), rf_params, cv=3, n_jobs=-1)
    rf_grid.fit(X_train_rf, y_train)
    rf = rf_grid.best_estimator_
    joblib.dump(rf, f"{save_path}/rf_{currency_name}.pkl")
    rf_preds = rf.predict(X_test_rf)
    log_model_performance(currency_name, "Random Forest", y_test, rf_preds)

    # --- SVR ---
    svr_params = {"C": [1, 10], "epsilon": [0.01, 0.1]}
    svr_grid = GridSearchCV(SVR(), svr_params, cv=3, n_jobs=-1)
    svr_grid.fit(X_train_rf, y_train)
    svr = svr_grid.best_estimator_
    joblib.dump(svr, f"{save_path}/svr_{currency_name}.pkl")
    svr_preds = svr.predict(X_test_rf)
    log_model_performance(currency_name, "SVR", y_test, svr_preds)

    # --- Linear Regression ---
    lr = LinearRegression()
    lr.fit(X_train_rf, y_train)
    joblib.dump(lr, f"{save_path}/lr_{currency_name}.pkl")
    lr_preds = lr.predict(X_test_rf)
    log_model_performance(currency_name, "Linear Regression", y_test, lr_preds)

    # --- LSTM ---
    X_train_lstm = X_lstm[:train_size]
    X_test_lstm = X_lstm[train_size:]
    y_train_scaled = y_train  # already normalized
    y_test_scaled = y_test

    best_mae = np.inf
    best_model = None
    for units in [32, 64]:
        for dropout in [0.1, 0.2]:
            for batch_size in [8, 16]:
                model = Sequential()
                model.add(LSTM(units, input_shape=(X_train_lstm.shape[1], 1)))
                model.add(Dropout(dropout))
                model.add(Dense(1))
                model.compile(loss='mse', optimizer='adam')
                early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

                model.fit(X_train_lstm, y_train_scaled, epochs=30, batch_size=batch_size,
                          validation_split=0.2, verbose=0, callbacks=[early_stop])
                
                preds = model.predict(X_test_lstm).ravel()
                mae = mean_absolute_error(y_test_scaled, preds)

                if mae < best_mae:
                    best_mae = mae
                    best_model = model
                    best_preds = preds

    best_model.save(f"{save_path}/lstm_{currency_name}.h5")
    joblib.dump(scaler, f"{save_path}/scaler_{currency_name}.pkl")
    log_model_performance(currency_name, "LSTM", y_test_scaled, best_preds)

    return True

# --- FX Dataset ---
df = load_fx_data()

from datetime import datetime

# Sidebar: Controls
st.sidebar.header(" Train FX Forecasting Models")
currency_option = st.sidebar.selectbox("Select Currency", df["Currency"].unique())
model_dir = st.sidebar.text_input("Model Save Path", value="models")
force_retrain = st.sidebar.checkbox("Force Retrain Models", value=False)

if st.sidebar.button(" Train & Save Models"):
    model_path_check = all([
        os.path.exists(f"{model_dir}/rf_{currency_option}.pkl"),
        os.path.exists(f"{model_dir}/svr_{currency_option}.pkl"),
        os.path.exists(f"{model_dir}/lr_{currency_option}.pkl"),
        os.path.exists(f"{model_dir}/lstm_{currency_option}.h5")
    ])
    if not model_path_check or force_retrain:
        with st.spinner(f"Training models for {currency_option}..."):
            success = train_and_save_models(currency_option, df, save_path=model_dir)
        if success:
            st.sidebar.success(f" Models for {currency_option} saved to `{model_dir}`")
            st.rerun()
    else:
        st.sidebar.info(" Models already exist. Use 'Force Retrain' to overwrite.")

# View log comparison
if st.sidebar.button(" View Model Feedback Log"):
    try:
        log_df = pd.read_csv("model_feedback_log.csv")
        st.sidebar.dataframe(log_df[log_df["Currency"] == currency_option])
    except Exception as e:
        st.sidebar.error("No log found yet.")




# Assign manual volatility group
def assign_manual_group(vol):
    if vol < 0.07:
        return "Group 1"
    elif vol < 0.50:
        return "Group 2"
    elif vol < 0.60:
        return "Group 3"
    else:
        return "Group 4"


@st.cache_data
def compute_thresholds_per_currency(df):
    summary = []
    for ccy, group in df.groupby("Currency"):
        # Volatility for dynamic models (unchanged)
        vol_series = group["Volatility"].dropna()
        avg_vol = vol_series.mean()

        # Manual group uses realized OHLC volatility
        realized_series = group["RealizedVol"].dropna()
        avg_realized_vol = realized_series.mean()

        # Manual group and threshold (based on OHLC-based logic)
        manual_group = assign_manual_group(avg_realized_vol)
        manual_threshold = {
            "Group 1": 0.10,
            "Group 2": 0.25,
            "Group 3": 0.55,
            "Group 4": 0.80
        }[manual_group]

        try:
            am = arch_model(vol_series, vol='GARCH', p=1, q=1)
            res = am.fit(disp="off")
            forecast = res.forecast(horizon=1).variance.values[-1][0] ** 0.5
        except:
            forecast = np.nan

        tail_data = vol_series[vol_series > vol_series.quantile(EVT_TAIL_PCT)]
        try:
            evt_params = genpareto.fit(tail_data)
            evt_threshold = genpareto.ppf(0.999, *evt_params)
        except:
            evt_threshold = np.nan

        summary.append({
            "Currency": ccy,
            "AvgVol": avg_vol,
            "AvgRealizedVol": avg_realized_vol,
            "ManualGroup": manual_group,
            "ManualThreshold": manual_threshold,
            "GARCH_Forecast": forecast,
            "95th_Pct": vol_series.quantile(PCT_THRESHOLD),
            "EVT_Threshold": evt_threshold
        })
    return pd.DataFrame(summary)

df_summary = compute_thresholds_per_currency(df)


# Tab layout
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "Overview", "Thresholds", "Live FX Rate Forecasting",
    "Final Results & Recommendation", "Cross-Currency View"
])

with tab1:
    st.header(" FX Volatility Monitoring: Manual vs Dynamic")

    st.markdown("""
    This app helps identify **abnormal FX behavior** by comparing:

    -  **Manual Thresholds**: Fixed bands, used operationally today.
    -  **Dynamic Thresholds**: Adjust to real market risk using GARCH, EVT, and percentiles.
    -  **Forecast Models**: Predict upcoming risk using AI/ML + time series methods.

    ---
    ###  Manual Threshold Issues
    - Static bands don't reflect current risk.
    - Cannot handle shocks (e.g. elections, inflation prints).
    - Trigger false alerts or miss critical spikes.

    ---
    ###  Dynamic Threshold Benefits
    - Adapted from statistical/ML models using rolling vol.
    - Explainable and validated per currency.
    - Improve alert accuracy and stakeholder trust.

    Use the tabs to explore per-currency insights.
    """)

with tab2:
    st.header(" Compare Thresholds: Manual vs Dynamic vs Technical")

    st.markdown("""
    This tab compares:
    -  Static **manual thresholds** from OHLC rules
    -  Dynamic models (GARCH, EVT, percentile)
    -  Technical thresholds (ATR, SuperTrend, Donchian)

    Stakeholders can see where manual logic fails to capture volatility shifts.
    """)
    selected_currency = st.selectbox("Select Currency", df["Currency"].unique(),index=None, key="tab1_currency")
    if selected_currency:
        df_currency = df[df["Currency"] == selected_currency].copy().sort_values("Date")

        st.subheader(f" {selected_currency} Close Price Trend")
        fig = px.line(df_currency, x="Date", y="Close")
        st.plotly_chart(fig, use_container_width=True)

        with st.expander("🔍 Explore KDE, Boxplot, and Seasonality"):
            fig_kde = px.histogram(df_currency, x='Close', histnorm='probability density')
            st.plotly_chart(fig_kde, use_container_width=True)

            fig_box = px.box(df_currency.melt(value_vars=["Open", "High", "Low", "Close"]),
                            x="variable", y="value", title="OHLC Distribution")
            st.plotly_chart(fig_box, use_container_width=True)

            try:
                decomposition = sm.tsa.seasonal_decompose(df_currency.set_index("Date")["Close"], model="additive", period=30)
                fig_decomp = go.Figure()
                fig_decomp.add_trace(go.Scatter(x=decomposition.trend.index, y=decomposition.trend, name="Trend"))
                fig_decomp.add_trace(go.Scatter(x=decomposition.seasonal.index, y=decomposition.seasonal, name="Seasonal"))
                fig_decomp.add_trace(go.Scatter(x=decomposition.resid.index, y=decomposition.resid, name="Residual"))
                fig_decomp.update_layout(title="Seasonal Decomposition")
                st.plotly_chart(fig_decomp, use_container_width=True)
            except Exception as e:
                st.warning(f"Decomposition error: {e}")
    st.dataframe(df_summary[[ "Currency", "AvgVol", "AvgRealizedVol",
                              "ManualGroup", "ManualThreshold", 
                              "GARCH_Forecast", "95th_Pct", "EVT_Threshold"]],
                 use_container_width=True)

    sel = st.selectbox("Select Currency", df_summary["Currency"], key="tab2_currency")
    series = df[df["Currency"] == sel].copy().sort_values("Date")
    series["LogReturn"] = np.log(series["Close"]).diff()

    tr1 = series["High"] - series["Low"]
    tr2 = (series["High"] - series["Close"].shift()).abs()
    tr3 = (series["Low"] - series["Close"].shift()).abs()
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    series["ATR"] = tr.rolling(window=14).mean()
    series["ATR_Threshold"] = series["ATR"] * 1.5
    series["DonchianUpper"] = series["High"].rolling(20).max()
    series["DonchianLower"] = series["Low"].rolling(20).min()
    hl2 = (series["High"] + series["Low"]) / 2
    series["SuperTrend"] = np.where(series["Close"] > hl2 - (2 * series["ATR"]),
                                    hl2 - (2 * series["ATR"]), hl2 + (2 * series["ATR"]))

    fig = px.line(series, x="Date", y="OHLCVolatility", title=f"{sel}: Volatility vs All Thresholds")

    for label in ["ManualThreshold", "GARCH_Forecast", "95th_Pct", "EVT_Threshold"]:
        val = df_summary[df_summary["Currency"] == sel][label].values[0]
        if pd.notnull(val):
            fig.add_hline(y=val, line_dash="dot", annotation_text=label)

    fig.add_trace(go.Scatter(x=series["Date"], y=series["ATR_Threshold"], name="ATR Threshold", line=dict(dash="dot")))
    fig.add_trace(go.Scatter(x=series["Date"], y=series["DonchianUpper"], name="Donchian Upper", line=dict(dash="dot")))
    fig.add_trace(go.Scatter(x=series["Date"], y=series["SuperTrend"], name="SuperTrend", line=dict(dash="dot")))

    st.plotly_chart(fig, use_container_width=True)

    with st.expander(" Last 30-Day Threshold Snapshot"):
        st.dataframe(series[[ "Date", "OHLCVolatility", "ATR_Threshold",
                              "DonchianUpper", "SuperTrend" ]].tail(30), use_container_width=True)
with tab3:
    st.header(" ML-Based FX Forecasting (Train/Test + Tuning + Explainability)")

    st.markdown("""
    This tab uses saved models (if available) or retrains on-the-fly if missing.  
    It compares:
    - Random Forest, SVR, Linear Regression, LSTM, XGBoost
    - LSTM shows confidence bands
    - SHAP explains RF predictions
    """)

    selected_currency = st.selectbox("Select Currency for Forecasting", df["Currency"].unique(),index=None, key="tab3_cur")
    forecast_horizon = st.slider("Forecast Horizon (Days)", 5, 60, 20)
    if selected_currency:
        df_cur = df[df["Currency"] == selected_currency][["Date", "Close"]].copy()
        df_cur["Date"] = pd.to_datetime(df_cur["Date"])
        df_cur.set_index("Date", inplace=True)

        # Create lag features
        for i in range(1, 8):
            df_cur[f"lag_{i}"] = df_cur["Close"].shift(i)
        df_cur.dropna(inplace=True)

        X = df_cur.drop(columns="Close")
        y = df_cur["Close"]
        X_train, X_test = X.iloc[:-forecast_horizon], X.iloc[-forecast_horizon:]
        y_train, y_test = y.iloc[:-forecast_horizon], y.iloc[-forecast_horizon:]

        # Scale
        scaler = MinMaxScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # Load saved models
        model_dict, saved_scaler = load_models(selected_currency)
        use_saved = model_dict is not None

        if use_saved:
            st.success(" Loaded saved models for forecasting.")

            rf_preds = model_dict["Random Forest"].predict(X_test_scaled)
            svr_preds = model_dict["SVR"].predict(X_test_scaled)
            lr_preds = model_dict["Linear Regression"].predict(X_test_scaled)

            # LSTM reshape
            X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))
            lstm = model_dict["LSTM"]
            lstm_preds_scaled = lstm.predict(X_test_lstm).ravel()
            lstm_preds = saved_scaler.inverse_transform(lstm_preds_scaled.reshape(-1, 1)).ravel()

        else:
            st.warning(" Saved models not found. Retraining from scratch...")

            # --- Model 1: Random Forest ---
            rf = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
            rf.fit(X_train_scaled, y_train)
            rf_preds = rf.predict(X_test_scaled)

            # --- Model 2: SVR ---
            svr = SVR(C=10, epsilon=0.1)
            svr.fit(X_train_scaled, y_train)
            svr_preds = svr.predict(X_test_scaled)

            # --- Model 3: Linear Regression ---
            lr = LinearRegression()
            lr.fit(X_train_scaled, y_train)
            lr_preds = lr.predict(X_test_scaled)

            # --- Model 4: LSTM ---
            y_scaler = MinMaxScaler()
            y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()
            X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
            X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

            lstm = Sequential()
            lstm.add(LSTM(64, input_shape=(X_train_lstm.shape[1], 1)))
            lstm.add(Dropout(0.2))
            lstm.add(Dense(1))
            lstm.compile(loss='mse', optimizer='adam')
            early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
            lstm.fit(X_train_lstm, y_train_scaled, epochs=25, batch_size=16, validation_split=0.2, verbose=0, callbacks=[early_stop])

            lstm_preds_scaled = lstm.predict(X_test_lstm).ravel()
            lstm_preds = y_scaler.inverse_transform(lstm_preds_scaled.reshape(-1, 1)).ravel()

        # --- Model 5: XGBoost ---
        xgb = XGBRegressor(n_estimators=100, max_depth=5, random_state=42, verbosity=0)
        xgb.fit(X_train_scaled, y_train)
        xgb_preds = xgb.predict(X_test_scaled)

        # --- Forecast Results ---
        results = pd.DataFrame({
            "Date": X_test.index,
            "Actual": y_test.values,
            "RandomForest": rf_preds,
            "SVR": svr_preds,
            "LinearRegression": lr_preds,
            "XGBoost": xgb_preds,
            "LSTM": lstm_preds
        }).set_index("Date")

        # Confidence Interval for LSTM
        lstm_std = np.std(y_train - lstm.predict(X_train_scaled.reshape(X_train_scaled.shape[0], -1, 1)).flatten())
        results["LSTM_upper"] = results["LSTM"] + 1.96 * lstm_std
        results["LSTM_lower"] = results["LSTM"] - 1.96 * lstm_std

        # --- Plot ---
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=results.index, y=results["Actual"], name="Actual", line=dict(color="white")))
        for col in ["RandomForest", "SVR", "LinearRegression", "XGBoost", "LSTM"]:
            fig.add_trace(go.Scatter(x=results.index, y=results[col], name=col))
        fig.add_trace(go.Scatter(
            x=results.index.tolist() + results.index[::-1].tolist(),
            y=results["LSTM_upper"].tolist() + results["LSTM_lower"][::-1].tolist(),
            fill='toself', fillcolor='rgba(0,255,255,0.2)',
            line=dict(color='rgba(255,255,255,0)'), name="LSTM CI", showlegend=False
        ))
        fig.update_layout(title=f"{selected_currency}: Model Forecasts vs Actual", xaxis_title="Date", yaxis_title="Close")
        st.plotly_chart(fig, use_container_width=True)

        # --- Metrics ---
        st.subheader(" Forecast Accuracy Metrics")
        def rmse(y_true, y_pred): return ((y_true - y_pred) ** 2).mean() ** 0.5
        metrics = {
            "Model": [],
            "MAE": [],
            "RMSE": [],
            "MAPE": []
        }
        for name, pred in [("Random Forest", rf_preds), ("SVR", svr_preds), ("Linear Regression", lr_preds),
                        ("XGBoost", xgb_preds), ("LSTM", lstm_preds)]:
            metrics["Model"].append(name)
            metrics["MAE"].append(mean_absolute_error(y_test, pred))
            metrics["RMSE"].append(rmse(y_test, pred))
            metrics["MAPE"].append(mean_absolute_percentage_error(y_test, pred))
        st.dataframe(pd.DataFrame(metrics).round(4), use_container_width=True)

        # --- SHAP for RF ---
        st.subheader(" SHAP Feature Importance (Random Forest)")
        explainer = shap.Explainer(model_dict["Random Forest"] if use_saved else rf, X_train_scaled)
        shap_values = explainer(X_test_scaled)
        fig, ax = plt.subplots()
        shap.plots.bar(shap_values, max_display=10, show=False, ax=ax)
        st.pyplot(fig)


# --- Tab 4: Final Thresholds & Forecast Accuracy (Enhanced) ---
with tab4:
    st.header("Final Thresholds & Forecast Accuracy")

    # Statistical final threshold
    df_summary["FinalThreshold"] = df_summary[["95th_Pct", "GARCH_Forecast", "EVT_Threshold"]].max(axis=1)

    # Load best RMSE models
    @st.cache_data
    def load_best_rmse_summary():
        try:
            log_df = pd.read_csv("model_feedback_log.csv")
            log_df["Currency"] = log_df["Currency"].str.strip().str.upper()
            log_df = log_df.dropna(subset=["RMSE"])
            best_model = log_df.sort_values("RMSE").drop_duplicates(subset=["Currency"])
            return best_model.rename(columns={"RMSE": "Best_RMSE"})[["Currency", "Model", "Best_RMSE"]]
        except Exception as e:
            st.error(f"Error loading RMSE summary: {e}")
            return pd.DataFrame(columns=["Currency", "Model", "Best_RMSE"])

    selected_currency = st.selectbox("Select Currency", df["Currency"].unique(), index=None, key="tab4_currency")
    if selected_currency:
        df_final = df_summary.merge(load_best_rmse_summary(), on="Currency", how="left")
        row = df_final[df_final["Currency"] == selected_currency].iloc[0]

        st.subheader(f"{selected_currency}: Thresholds & Model Summary")
        st.markdown(f"""
        - **Avg Log-Return Volatility** (Dynamic): `{round(row['AvgVol'], 4)}`
        - **Avg OHLC Realized Volatility** (Manual): `{round(row['AvgRealizedVol'], 4)}`
        - **Manual Threshold**: `{round(row['ManualThreshold'], 4)}`
        - **Final Dynamic Threshold** (Statistical): `{round(row['FinalThreshold'], 4)}`
        - **Best Forecasting Model**: `{row['Model'] if pd.notnull(row['Model']) else 'NA'}`
        - **Best Model RMSE**: `{round(row['Best_RMSE'], 4) if pd.notnull(row['Best_RMSE']) else 'NA'}`
        """)

        #  New Enhanced Volatility Comparison Plot
        st.markdown("###  Volatility Trends vs Thresholds")

        trend_df = df[df["Currency"] == selected_currency].copy().sort_values("Date")
        dates = trend_df["Date"]

        fig_vol = go.Figure()
        fig_vol.add_trace(go.Scatter(x=dates, y=trend_df["RealizedVol"], name="OHLC Realized Vol (Manual)", line=dict(color="orange")))
        fig_vol.add_trace(go.Scatter(x=dates, y=trend_df["Volatility"], name="Log-Return Volatility (Dynamic)", line=dict(color="white")))

        fig_vol.add_hline(y=row["ManualThreshold"], line_color="orange", line_dash="dot", annotation_text="Manual Threshold", annotation_position="top left")
        fig_vol.add_hline(y=row["FinalThreshold"], line_color="green", line_dash="dash", annotation_text="Statistical Threshold", annotation_position="top left")

        fig_vol.update_layout(title="Volatility Comparison: Realized vs Log-Return",
                              xaxis_title="Date", yaxis_title="Volatility", height=450)
        st.plotly_chart(fig_vol, use_container_width=True)

        st.markdown("""
        **Interpretation Tip:**  
        - Manual thresholds use slower **realized OHLC volatility**.  
        - Dynamic thresholds use faster **log-return volatility**, helping catch spikes sooner.  
        - If realized breaches but log-return doesn't (or vice versa), it signals mismatch and latency in manual rules.
        """)

        # Threshold comparison bar chart
        fig = go.Figure()
        fig.add_trace(go.Bar(name="Manual", x=[selected_currency], y=[row["ManualThreshold"]]))
        fig.add_trace(go.Bar(name="Statistical", x=[selected_currency], y=[row["FinalThreshold"]]))
        if pd.notnull(row["Best_RMSE"]):
            fig.add_trace(go.Bar(name="Best RMSE", x=[selected_currency], y=[row["Best_RMSE"]]))
        fig.update_layout(title="Threshold & Model Comparison", barmode="group")
        st.plotly_chart(fig, use_container_width=True)

        # Global Threshold Table
        st.subheader(" Cross-Currency Threshold Overview")
        st.dataframe(row[[
            "Currency", "ManualThreshold", "FinalThreshold", "Model", "Best_RMSE"
        ]], use_container_width=True)

        st.download_button(" Download Final Summary", df_final.to_csv(index=False),
                           file_name="final_thresholds_summary.csv")
with tab5:
    st.header(" Cross-Currency Thresholds (Synthetic FX)")

    st.markdown("""
    This module estimates thresholds for cross-currency pairs based on synthetic construction.  
    - **Synthetic Rate**: `BaseUSD / QuoteUSD`  
    - **Volatility Measures**:  
      - Manual = average realized volatility (OHLC-based)  
      - Dynamic = log-return rolling volatility  
    - **Thresholds**:  
      - Manual = max(manual thresholds of base & quote legs)  
      - Dynamic = 95th percentile of synthetic volatility  
    """)

    currencies = df["Currency"].unique().tolist()
    cross_pairs = [(f"{b}{q}", b, q) for b in currencies for q in currencies if b != q]
    pair_labels = [f"{p[0]} = {p[1]}USD × USD{p[2]}" for p in cross_pairs]
    pair_choice = st.selectbox("Select Cross Pair", pair_labels,index=None)
    if pair_choice:
        selected_pair = cross_pairs[pair_labels.index(pair_choice)]
        pair_name, base_ccy, quote_ccy = selected_pair

        try:
            base_df = df[df["Currency"] == base_ccy][["Date", "Close"]].copy()
            quote_df = df[df["Currency"] == quote_ccy][["Date", "Close"]].copy()

            merged = pd.merge(base_df, quote_df, on="Date", suffixes=(f"_{base_ccy}", f"_{quote_ccy}")).dropna()
            merged["SyntheticRate"] = merged[f"Close_{base_ccy}"] / merged[f"Close_{quote_ccy}"]
            merged["LogReturn"] = np.log(merged["SyntheticRate"]).diff()

            merged["CrossVolatility"] = merged["LogReturn"].rolling(ROLL_WINDOW).std() * ANNUALIZE
            merged["ManualVolatility"] = merged["SyntheticRate"].rolling(ROLL_WINDOW).std() / np.sqrt(252)
            merged.dropna(inplace=True)

            # Pull leg-level manual thresholds
            df_thresh = df_summary.set_index("Currency")
            base_thresh = df_thresh.loc[base_ccy, "ManualThreshold"]
            quote_thresh = df_thresh.loc[quote_ccy, "ManualThreshold"]
            manual_cross = max(base_thresh, quote_thresh)
            dynamic_cross = merged["CrossVolatility"].quantile(PCT_THRESHOLD)

            # Breach flags
            merged["Breach_Manual"] = merged["CrossVolatility"] > manual_cross
            merged["Breach_Dynamic"] = merged["CrossVolatility"] > dynamic_cross

            # Compute vol stats
            manual_vol_avg = merged["ManualVolatility"].mean()
            dynamic_vol_avg = merged["CrossVolatility"].mean()

            #  Threshold + volatility summary
            st.subheader(f"🔍 {pair_name} Threshold & Volatility Summary")
            st.markdown(f"""
            - **Base Currency** `{base_ccy}` Manual Threshold: `{base_thresh:.4f}`  
            - **Quote Currency** `{quote_ccy}` Manual Threshold: `{quote_thresh:.4f}`  
            - **→ Manual Cross Threshold**: `{manual_cross:.4f}`  
            - **→ Dynamic Threshold** (95th pct): `{dynamic_cross:.4f}`  

            ---
            - **Avg Manual Volatility** (Synthetic Rate std / √252): `{manual_vol_avg:.4f}`  
            - **Avg Dynamic Volatility** (Rolling log-return std): `{dynamic_vol_avg:.4f}`
            """)

            #  Volatility chart
            fig = px.line(merged, x="Date", y="CrossVolatility", title=f"{pair_name} Synthetic Volatility vs Thresholds")
            fig.add_hline(y=manual_cross, line_color="red", line_dash="dot", annotation_text="Manual Threshold")
            fig.add_hline(y=dynamic_cross, line_color="green", line_dash="dash", annotation_text="Dynamic Threshold")

            fig.add_trace(go.Scatter(
                x=merged[merged["Breach_Dynamic"]]["Date"],
                y=merged[merged["Breach_Dynamic"]]["CrossVolatility"],
                mode="markers", name="Breach (Dynamic)", marker=dict(color="orange", size=8, symbol="x")
            ))

            fig.add_trace(go.Scatter(
                x=merged[merged["Breach_Manual"]]["Date"],
                y=merged[merged["Breach_Manual"]]["CrossVolatility"],
                mode="markers", name="Breach (Manual)", marker=dict(color="red", size=7, symbol="circle")
            ))

            st.plotly_chart(fig, use_container_width=True)

            st.markdown("###  Last 30-Day Snapshot")
            st.dataframe(
                merged[["Date", "SyntheticRate", "CrossVolatility", "ManualVolatility", "Breach_Manual", "Breach_Dynamic"]].tail(30),
                use_container_width=True
            )

        except Exception as e:
            st.error(f"Failed to compute synthetic cross pair volatility: {e}")
