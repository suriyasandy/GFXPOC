# --- PART 1: Imports, Global Configs, and Utility Caching ---
import streamlit as st
import pandas as pd
import numpy as np
import os, joblib
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
from arch import arch_model
from scipy.stats import genpareto, kurtosis, skew
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
import keras
from keras.layers import Conv1D, MaxPooling1D, Flatten
from keras.models import Sequential
from keras.layers import LSTM, Dropout, Dense
from keras.callbacks import EarlyStopping
from keras.models import load_model
from prophet import Prophet
from statsmodels.tsa.stattools import adfuller
import statsmodels.api as sm
from xgboost import XGBRegressor

ROLL_WINDOW = 60
ANNUALIZE = np.sqrt(252)
PCT_THRESHOLD = 0.95
EVT_TAIL_PCT = 0.995

# Page config
st.set_page_config(page_title="FX Volatility App", layout="wide")

# Background CSS
st.markdown("""
<style>
.stApp { background-color: light-gray; }
</style>
""", unsafe_allow_html=True)

# --- FX Data Loader ---
@st.cache_data
def load_fx_data():
    df = pd.read_csv("reuters_fx_data.csv")
    df["Date"] = pd.to_datetime(df["Date"])
    df.sort_values(["Currency", "Date"], inplace=True)
    df["LogReturn"] = df.groupby("Currency")["Close"].transform(lambda x: np.log(x).diff())
    df["Volatility"] = df.groupby("Currency")["LogReturn"].transform(lambda x: x.rolling(ROLL_WINDOW).std()) * ANNUALIZE
    df["RealizedVol"] = df["OHLCVolatility"] / np.sqrt(252)
    return df.dropna()

# --- Sequence Creator for ML Models ---
def create_sequences(data, window):
    X, y = [], []
    if isinstance(data, pd.Series):
        data = data.dropna().values
    else:
        data = data[~np.isnan(data).any(axis=1)]
    for i in range(len(data) - window):
        X.append(data[i:i+window])
        y.append(data[i+window])
    return np.array(X), np.array(y)

# --- Model Loader ---
def load_models(currency_name, path="models"):
    try:
        rf = joblib.load(f"{path}/rf_{currency_name}.pkl")
        svr = joblib.load(f"{path}/svr_{currency_name}.pkl")
        lr = joblib.load(f"{path}/lr_{currency_name}.pkl")
        lstm = load_model(f"{path}/lstm_{currency_name}.h5", compile=False)
        scaler = joblib.load(f"{path}/scaler_{currency_name}.pkl")
        return {"Random Forest": rf, "SVR": svr, "Linear Regression": lr, "LSTM": lstm}, scaler
    except Exception as e:
        st.warning(f"Model loading failed for {currency_name}: {e}")
        return None, None

# --- Model Training and Saving ---
def train_and_save_models(currency_name, df, save_path="models", window=30):
    os.makedirs(save_path, exist_ok=True)
    df_currency = df[df["Currency"] == currency_name][["Date", "Close"]].dropna().copy()
    df_currency["Date"] = pd.to_datetime(df_currency["Date"])
    df_currency.set_index("Date", inplace=True)

    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(df_currency)

    X, y = create_sequences(scaled_data, window)
    X_rf = X.reshape(X.shape[0], -1)
    X_lstm = X.reshape(X.shape[0], X.shape[1], 1)
    y = y.ravel()

    rf_params = {"n_estimators": [50, 100], "max_depth": [5, 10, None]}
    rf = GridSearchCV(RandomForestRegressor(random_state=42), rf_params, cv=TimeSeriesSplit(n_splits=3))
    rf.fit(X_rf, y)
    joblib.dump(rf.best_estimator_, f"{save_path}/rf_{currency_name}.pkl")

    svr_params = {"C": [1, 10], "gamma": ["scale", "auto"], "epsilon": [0.01, 0.1]}
    svr = GridSearchCV(SVR(), svr_params, cv=TimeSeriesSplit(n_splits=3))
    svr.fit(X_rf, y)
    joblib.dump(svr.best_estimator_, f"{save_path}/svr_{currency_name}.pkl")

    lr = LinearRegression()
    lr.fit(X_rf, y)
    joblib.dump(lr, f"{save_path}/lr_{currency_name}.pkl")

    lstm = Sequential()
    lstm.add(LSTM(64, input_shape=(X_lstm.shape[1], X_lstm.shape[2])))
    lstm.add(Dropout(0.2))
    lstm.add(Dense(1))
    lstm.compile(loss='mse', optimizer='adam')

    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    lstm.fit(X_lstm, y, epochs=20, batch_size=16, verbose=0, callbacks=[early_stop], validation_split=0.2)

    lstm.save(f"{save_path}/lstm_{currency_name}.h5")

    joblib.dump(scaler, f"{save_path}/scaler_{currency_name}.pkl")
    return True

# --- FX Dataset ---
df = load_fx_data()

# --- Sidebar UI ---
st.sidebar.header("Train FX Forecasting Models")
currency_option = st.sidebar.selectbox("Select Currency", df["Currency"].unique().tolist())

# Add editable model save path
model_dir = st.sidebar.text_input("Model Save Path", value="models")

# Add training + rerun logic
if st.sidebar.button("Train & Save Models"):
    with st.spinner(f"Training models for {currency_option}..."):
        success = train_and_save_models(currency_option, df, save_path=model_dir)
    if success:
        st.sidebar.success(f"Models for {currency_option} saved to `{model_dir}`")
        if st.sidebar.button("🔄 Reload App"):
            st.rerun()



# Assign manual volatility group
def assign_manual_group(vol):
    if vol < 0.07:
        return "Group 1"
    elif vol < 0.50:
        return "Group 2"
    elif vol < 0.60:
        return "Group 3"
    else:
        return "Group 4"

# --- PART 2: Threshold Summary (with Caching) and Prophet Optimization ---

@st.cache_data
def compute_thresholds_per_currency(df):
    summary = []
    for ccy, group in df.groupby("Currency"):
        # Volatility for dynamic models (unchanged)
        vol_series = group["Volatility"].dropna()
        avg_vol = vol_series.mean()

        # Manual group uses realized OHLC volatility
        realized_series = group["RealizedVol"].dropna()
        avg_realized_vol = realized_series.mean()

        # Manual group and threshold (based on OHLC-based logic)
        manual_group = assign_manual_group(avg_realized_vol)
        manual_threshold = {
            "Group 1": 0.10,
            "Group 2": 0.25,
            "Group 3": 0.55,
            "Group 4": 0.80
        }[manual_group]

        try:
            am = arch_model(vol_series, vol='GARCH', p=1, q=1)
            res = am.fit(disp="off")
            forecast = res.forecast(horizon=1).variance.values[-1][0] ** 0.5
        except:
            forecast = np.nan

        tail_data = vol_series[vol_series > vol_series.quantile(EVT_TAIL_PCT)]
        try:
            evt_params = genpareto.fit(tail_data)
            evt_threshold = genpareto.ppf(0.999, *evt_params)
        except:
            evt_threshold = np.nan

        summary.append({
            "Currency": ccy,
            "AvgVol": avg_vol,
            "AvgRealizedVol": avg_realized_vol,
            "ManualGroup": manual_group,
            "ManualThreshold": manual_threshold,
            "GARCH_Forecast": forecast,
            "95th_Pct": vol_series.quantile(PCT_THRESHOLD),
            "EVT_Threshold": evt_threshold
        })
    return pd.DataFrame(summary)

df_summary = compute_thresholds_per_currency(df)

# --- Stationarity Check ---
def adf_stationarity_test(series):
    result = adfuller(series.dropna())
    return {
        "ADF Statistic": result[0],
        "p-value": result[1],
        "IsStationary": result[1] < 0.05
    }

# --- Prophet Forecast (Univariate) ---
@st.cache_data
def prophet_forecast_univariate(currency_data):
    df_prophet = currency_data[["Date", "Close"]].dropna().rename(columns={"Date": "ds", "Close": "y"})
    model = Prophet(seasonality_mode="multiplicative", daily_seasonality=True)
    model.fit(df_prophet)
    future = model.make_future_dataframe(periods=15)
    forecast = model.predict(future)
    return forecast, model

# --- Prophet Forecast (Multivariate) ---
@st.cache_data
def prophet_forecast_multivariate(currency_data):
    df_prophet = currency_data[["Date", "Close", "High", "Low"]].dropna().rename(
        columns={"Date": "ds", "Close": "y", "High": "add1", "Low": "add2"})
    model = Prophet(seasonality_mode="multiplicative", daily_seasonality=True)
    model.add_regressor("add1")
    model.add_regressor("add2")
    model.fit(df_prophet)
    future = df_prophet.drop(columns=["y"])
    forecast = model.predict(future)
    return forecast, model

# --- Seasonal Decomposition ---
def seasonal_decomposition_plot(df, currency, freq="W"):
    sub_df = df[df["Currency"] == currency][["Date", "Close"]].dropna()
    series = sub_df.set_index("Date")["Close"].resample(freq).mean().ffill()

    min_required = {"W": 104, "D": 730, "M": 24}[freq]
    if len(series) < min_required:
        st.warning(f"Not enough data for {freq} decomposition (found {len(series)}, need ≥ {min_required})")
        return

    result = sm.tsa.seasonal_decompose(series, model='multiplicative')
    fig = result.plot()
    plt.suptitle(f"{currency} Seasonal Decomposition ({freq})")
    st.pyplot(fig)
# --- PART 3: Tab 1 (Overview) and Tab 2 (Threshold Viewer) ---

# Tab layout
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "Overview", "Thresholds", "Live FX Rate Forecasting",
    "Final Results & Recommendation", "Cross-Currency View"
])

# --- Tab 1: Overview ---
# --- Tab 1: Dashboard Overview & Explainability ---
with tab1:
    st.header("FX Threshold Monitoring: Overview")

    st.markdown("""
    This dashboard highlights the difference between manual vs dynamic thresholds for FX volatility monitoring.
    
    - **Manual Thresholds** are based on static bands from historical realized volatility.
    - **Dynamic Thresholds** adapt using statistical models (GARCH, EVT, Percentile).
    - **Forecast Accuracy** is benchmarked across ML and time-series models (Tab 3).
    
    Select a currency below to explore its behavior and interpretability.
    """)

    selected_currency = st.selectbox("Select Currency", df["Currency"].unique(), key="tab1_currency")
    df_currency = df[df["Currency"] == selected_currency].sort_values("Date").copy()
    df_currency["Date"] = pd.to_datetime(df_currency["Date"])

    st.subheader(f"📈 Historical Close Price for {selected_currency}")
    fig_close = px.line(df_currency, x="Date", y="Close", title=f"{selected_currency} Close Price Trend")
    st.plotly_chart(fig_close, use_container_width=True)

    with st.expander("🔍 Exploratory Analysis: KDE, Boxplot, and Seasonality"):
        # KDE
        fig_kde = px.histogram(df_currency, x='Close', histnorm='probability density', nbins=30)
        kde_fit = sm.nonparametric.KDEUnivariate(df_currency["Close"].dropna())
        kde_fit.fit()
        x_vals = np.linspace(df_currency["Close"].min(), df_currency["Close"].max(), 200)
        fig_kde.add_trace(go.Scatter(x=x_vals, y=kde_fit.evaluate(x_vals), mode="lines", name="KDE", line=dict(color="red")))
        fig_kde.update_layout(title="KDE + Histogram of Close Prices", xaxis_title="Close", yaxis_title="Density")
        st.plotly_chart(fig_kde, use_container_width=True)

        # Boxplot
        fig_box = px.box(df_currency.melt(value_vars=["Open", "High", "Low", "Close"]), x="variable", y="value",
                         title="OHLC Price Distribution")
        st.plotly_chart(fig_box, use_container_width=True)

        # Decomposition
        st.write("**Seasonal Decomposition (Additive):**")
        try:
            decomposition = sm.tsa.seasonal_decompose(df_currency["Close"], model="additive", period=30)
            fig_decomp = go.Figure()
            fig_decomp.add_trace(go.Scatter(x=df_currency["Date"], y=decomposition.trend, name="Trend"))
            fig_decomp.add_trace(go.Scatter(x=df_currency["Date"], y=decomposition.seasonal, name="Seasonal"))
            fig_decomp.add_trace(go.Scatter(x=df_currency["Date"], y=decomposition.resid, name="Residual"))
            fig_decomp.update_layout(title="Seasonal Decomposition", xaxis_title="Date", yaxis_title="Component")
            st.plotly_chart(fig_decomp, use_container_width=True)
        except Exception as e:
            st.warning(f"Decomposition failed: {e}")

    with st.expander("📊 Autocorrelation and Lag Plot"):
        from statsmodels.graphics.tsaplots import plot_acf
        fig_acf = plot_acf(df_currency["Close"], lags=40)
        st.pyplot(fig_acf.figure)

        df_currency["Lag1"] = df_currency["Close"].shift(1)
        fig_lag = px.scatter(df_currency, x="Lag1", y="Close", title="Lag-1 vs Close")
        st.plotly_chart(fig_lag, use_container_width=True)


# --- Tab 2: Threshold Comparison ---#
with tab2:
    st.header("Threshold Comparison Summary")

    st.dataframe(df_summary[[
        "Currency", "AvgVol", "AvgRealizedVol", "ManualGroup",
        "ManualThreshold", "GARCH_Forecast", "95th_Pct", "EVT_Threshold"
    ]], use_container_width=True)

    sel = st.selectbox("Select Currency", df_summary["Currency"], key="tab2_currency")
    series = df[df["Currency"] == sel].copy()
    summary = df_summary[df_summary["Currency"] == sel].iloc[0]

    fig = px.line(series, x="Date", y="Volatility", title=f"{sel} Volatility & Thresholds (LogReturn-based)")
    for label in ["ManualThreshold", "GARCH_Forecast", "95th_Pct", "EVT_Threshold"]:
        val = summary[label]
        if pd.notnull(val):
            fig.add_hline(y=val, line_dash="dot", annotation_text=label)

    # Overlay Ops-style volatility (Realized)
    fig.add_trace(go.Scatter(
        x=series["Date"], y=series["RealizedVol"],
        name="RealizedVol (Ops-style)", line=dict(dash="dash", color="orange")
    ))

    st.plotly_chart(fig, use_container_width=True)

    st.markdown(f"""
    **Notes**:
    - `Volatility`: Based on log-returns (used in GARCH, EVT, 95th).
    - `RealizedVol`: Based on OHLC, used by Ops for manual grouping.
    - `ManualThreshold`: Based on `RealizedVol` average.
    """)



# --- Tab 3: Forecast with Dynamic Test Period and Multiple Tuned Models ---
with tab3:
    st.header("Live FX Forecasting (Flexible Test Period + Tuned Models)")

    selected_currency = st.selectbox("Select Currency", df["Currency"].unique(), index=None, key="fx_forecast_currency")
    test_days = st.slider("Select Test Period (days before latest date)", min_value=30, max_value=90, step=30, value=60)
    selected_models = st.multiselect("Select Forecasting Models", ["Random Forest", "Linear Regression", "LSTM", "XGBoost", "CNN", "ARIMA", "SARIMA"], default=["LSTM", "XGBoost"])

    if selected_currency:
        df_ccy = df[df["Currency"] == selected_currency][["Date", "Close"]].dropna().copy()
        df_ccy["Date"] = pd.to_datetime(df_ccy["Date"])
        df_ccy.set_index("Date", inplace=True)

        latest_date = df_ccy.index.max()
        test_start = latest_date - pd.Timedelta(days=test_days)
        train_df = df_ccy[df_ccy.index < test_start]
        test_df = df_ccy[df_ccy.index >= test_start]

        if train_df.empty or test_df.empty:
            st.error("Not enough data for selected test period.")
        else:
            scaler = MinMaxScaler()
            scaled_all = scaler.fit_transform(df_ccy)
            scaled_train = scaler.transform(train_df)
            scaled_test = scaler.transform(test_df)

            def create_sequences(data, window=30):
                X, y = [], []
                for i in range(len(data) - window):
                    X.append(data[i:i+window])
                    y.append(data[i+window])
                return np.array(X), np.array(y)

            WINDOW = 30
            X_train, y_train = create_sequences(scaled_train, WINDOW)
            X_rf_test = []
            for i in range(len(test_df)):
                X_rf_test.append(scaled_all[-(len(test_df) + WINDOW - i):-len(test_df) + i])
            X_rf_test = np.array(X_rf_test).reshape(len(X_rf_test), -1)
            X_train_rf = X_train.reshape(X_train.shape[0], -1)
            X_train_lstm = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_lstm_test = np.array([x.reshape(WINDOW, 1) for x in X_rf_test])

            from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
            model_outputs_test, metrics = {}, {}
            y_true_test = test_df["Close"].values

            for model in selected_models:
                try:
                    if model == "Random Forest":
                        rf_params = {"n_estimators": [50, 100], "max_depth": [5, 10, None]}
                        rf = GridSearchCV(RandomForestRegressor(random_state=42), rf_params, cv=TimeSeriesSplit(n_splits=3))
                        rf.fit(X_train_rf, y_train.ravel())
                        preds = rf.predict(X_rf_test)
                        preds_train = rf.predict(X_train_rf)

                    elif model == "Linear Regression":
                        lr = LinearRegression()
                        lr.fit(X_train_rf, y_train)
                        preds = lr.predict(X_rf_test)
                        preds_train = lr.predict(X_train_rf)

                    elif model == "XGBoost":
                        xgb_params = {
                            "n_estimators": [50, 100],
                            "learning_rate": [0.05, 0.1],
                            "max_depth": [3, 5]
                        }
                        xgb = GridSearchCV(XGBRegressor(random_state=42, verbosity=0), xgb_params, cv=TimeSeriesSplit(n_splits=3))
                        xgb.fit(X_train_rf, y_train)
                        preds = xgb.predict(X_rf_test)
                        preds_train = xgb.predict(X_train_rf)

                    elif model == "LSTM":
                        lstm = Sequential()
                        lstm.add(LSTM(64, input_shape=(WINDOW, 1)))
                        lstm.add(Dropout(0.2))
                        lstm.add(Dense(1))
                        lstm.compile(optimizer="adam", loss="mse")
                        lstm.fit(X_train_lstm, y_train, epochs=20, batch_size=16, verbose=0)
                        preds = lstm.predict(X_lstm_test).ravel()
                        preds_train = lstm.predict(X_train_lstm).ravel()

                    elif model == "CNN":
                        from keras.layers import Conv1D, MaxPooling1D, Flatten
                        cnn = Sequential([
                            Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(WINDOW, 1)),
                            MaxPooling1D(pool_size=2),
                            Flatten(),
                            Dense(50, activation='relu'),
                            Dense(1)
                        ])
                        cnn.compile(optimizer='adam', loss='mse')
                        cnn.fit(X_train_lstm, y_train, epochs=20, batch_size=16, verbose=0)
                        preds = cnn.predict(X_lstm_test).flatten()
                        preds_train = cnn.predict(X_train_lstm).flatten()

                    elif model == "ARIMA":
                        from statsmodels.tsa.arima.model import ARIMA
                        arima_model = ARIMA(train_df["Close"], order=(0, 1, 1)).fit()
                        preds = arima_model.forecast(steps=len(test_df)).values
                        preds_train = arima_model.predict(start=1, end=len(train_df)-1).values

                    elif model == "SARIMA":
                        from statsmodels.tsa.statespace.sarimax import SARIMAX
                        sarima_model = SARIMAX(train_df["Close"], order=(0, 1, 1), seasonal_order=(1, 1, 1, 12)).fit(disp=False)
                        preds = sarima_model.forecast(steps=len(test_df)).values
                        preds_train = sarima_model.predict(start=1, end=len(train_df)-1).values

                    # Inverse scale (except ARIMA/SARIMA)
                    if model in ["ARIMA", "SARIMA"]:
                        model_outputs_test[model] = preds
                        model_outputs_test[f"{model}_train"] = preds_train
                    else:
                        model_outputs_test[model] = scaler.inverse_transform(preds.reshape(-1, 1)).flatten()
                        model_outputs_test[f"{model}_train"] = scaler.inverse_transform(preds_train.reshape(-1, 1)).flatten()

                    metrics[model] = {
                        "RMSE": np.sqrt(mean_squared_error(y_true_test, model_outputs_test[model])),
                        "MAPE": mean_absolute_percentage_error(y_true_test, model_outputs_test[model]) * 100
                    }

                except Exception as e:
                    st.warning(f"{model} failed: {e}")
                    continue


                fig = go.Figure()

                # 1. Plot Training Actual
                fig.add_trace(go.Scatter(
                    x=train_df.index,
                    y=train_df["Close"],
                    name="Training Actual",
                    line=dict(color="lightblue", width=2)
                ))

                # 2. Plot Test Actual
                fig.add_trace(go.Scatter(
                    x=test_df.index,
                    y=y_true_test,
                    name="Test Actual",
                    line=dict(color="white", width=2)
                ))

                # 3. Forecast per model on test set
                for model, forecast in model_outputs_test.items():
                    if not model.endswith("_train"):
                        fig.add_trace(go.Scatter(
                            x=test_df.index,
                            y=forecast,
                            name=f"{model} Forecast (Test)",
                            line=dict(dash="dot"),
                            mode="lines"
                        ))

                # 4. Forecasts per model on training set
                for key in model_outputs_test:
                    if key.endswith("_train"):
                        model_name = key.replace("_train", "")
                        y_train_pred = model_outputs_test[key]
                        x_train_range = train_df.index[-len(y_train_pred):]

                        fig.add_trace(go.Scatter(
                            x=x_train_range,
                            y=y_train_pred,
                            name=f"{model_name} Forecast (Train)",
                            line=dict(dash="dot", width=1),
                            opacity=0.5
                        ))

                # 5. Add shaded region for test set
                split_date = pd.to_datetime(test_df.index[0]).to_pydatetime()
                test_end_date = pd.to_datetime(test_df.index[-1]).to_pydatetime()

                fig.add_vrect(
                    x0=split_date,
                    x1=test_end_date,
                    fillcolor="rgba(255, 99, 71, 0.1)",  # tomato light
                    layer="below",
                    line_width=0,
                    annotation_text="Test Period",
                    annotation_position="top left"
                )

                fig.update_layout(
                    title=f"{selected_currency} Model Forecasts vs Actual (Train/Test Split)",
                    xaxis_title="Date",
                    yaxis_title="Close Price",
                    legend_title="Legend",
                    plot_bgcolor="rgba(0,0,0,0)",
                    paper_bgcolor="rgba(0,0,0,0)"
                )

                st.plotly_chart(fig, use_container_width=True)


            st.subheader("Forecast Accuracy")
            metric_df = pd.DataFrame(metrics).T.reset_index().rename(columns={"index": "Model"})
            st.dataframe(metric_df.style.format({"RMSE": "{:.4f}", "MAPE": "{:.2f}"}), use_container_width=True)

            best_model = metric_df.sort_values("MAPE").iloc[0]["Model"]
            best_forecast = model_outputs_test[best_model]
            best_mape = metric_df.sort_values("MAPE").iloc[0]["MAPE"]

            st.success(f"📈 Best model: **{best_model}** with MAPE: **{best_mape:.2f}%**")

            # Save forecast to CSV
            forecast_df = pd.DataFrame({
                "Date": test_df.index.strftime('%Y-%m-%d'),
                "Actual": y_true_test,
                f"{best_model}_Forecast": best_forecast
            })
            forecast_file = f"{selected_currency}_{best_model}_forecast.csv"
            st.download_button("Download Forecast CSV", forecast_df.to_csv(index=False), file_name=forecast_file)

            # --- New: Log model performance to CSV ---
            log_entry = {
                "Currency": selected_currency,
                "Model": best_model,
                "RMSE": metrics[best_model]["RMSE"],
                "MAPE": best_mape,
                "RunDate": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")
            }

            log_df = pd.DataFrame([log_entry])
            log_file = "model_feedback_log.csv"

            try:
                if os.path.exists(log_file):
                    existing = pd.read_csv(log_file)
                    updated = pd.concat([existing, log_df], ignore_index=True)
                else:
                    updated = log_df

                updated.to_csv(log_file, index=False)
                st.info("✅ Logged model performance to `model_feedback_log.csv`")
            except Exception as e:
                st.warning(f"⚠️ Failed to log model performance: {e}")


# --- Tab 4: Final Thresholds & Forecast Accuracy (with Model Comparison) ---
with tab4:
    st.header("Final Thresholds & Forecast Accuracy")

    df_summary["FinalThreshold"] = df_summary[[
         "95th_Pct", "GARCH_Forecast", "EVT_Threshold"
    ]].max(axis=1)

    @st.cache_data
    def load_best_mape_summary():
        try:
            log_df = pd.read_csv("model_feedback_log.csv")

            # Normalize Currency name
            log_df["Currency"] = log_df["Currency"].str.strip().str.upper()

            # Sort by run date and take the most recent per currency
            latest = log_df.sort_values("RunDate", ascending=False).drop_duplicates(subset=["Currency"])

            # Rename columns
            latest = latest.rename(columns={"MAPE": "Best_MAPE"})

            return latest[["Currency", "Best_MAPE", "Model"]]
        except Exception as e:
            st.error(f"Error loading MAPE summary: {e}")
            return pd.DataFrame(columns=["Currency", "Best_MAPE", "Model"])
              

    best_mape_df = load_best_mape_summary()
    df_final = df_summary.merge(best_mape_df, on="Currency", how="left")
    st.write("Log currencies:", best_mape_df["Currency"].unique())
    st.write("Summary currencies:", df_summary["Currency"].unique())
    st.write("Best model merge result:", df_final[df_final["Currency"] == selected_currency])
    selected_currency = st.selectbox("Select Currency", df_final["Currency"].unique(),index=None,key="tab4_currency")
    if selected_currency:
        row = df_final[df_final["Currency"] == selected_currency].iloc[0]

        st.subheader(f"{selected_currency}: Thresholds & Model Summary")
        st.markdown(f"""
        - **Avg Log-Return Volatility** (Dynamic): `{round(row['AvgVol'], 4)}`
        - **Avg OHLC Realized Volatility** (Manual): `{round(row['AvgRealizedVol'], 4)}`
        - **Manual Group (based on OHLC)**: `{row['ManualGroup']}`
        - **Manual Threshold**: `{round(row['ManualThreshold'], 4)}`
        - **Final Dynamic Threshold** (Max of GARCH, EVT, 95th): `{round(row['FinalThreshold'], 4)}`
        - **Best Forecasting Model**: `{row['Model'] if pd.notnull(row['Model']) else 'NA'}`
        - **Best Model MAPE**: `{round(row['Best_MAPE'], 2) if pd.notnull(row['Best_MAPE']) else 'NA'}%`
        """)
        if row["Best_MAPE"] > 10:
            st.warning(f"⚠️ Model performance warning: Best model MAPE is above 10%. Consider re-evaluation.")

        # Bar Chart: Manual vs Final Threshold
        fig = go.Figure()
        fig.add_trace(go.Bar(name="Manual", x=[selected_currency], y=[row["ManualThreshold"]]))
        fig.add_trace(go.Bar(name="Final Dynamic", x=[selected_currency], y=[row["FinalThreshold"]]))
        if pd.notnull(row["Best_MAPE"]):
            fig.add_trace(go.Bar(name="Best Model MAPE", x=[selected_currency], y=[row["Best_MAPE"] / 100]))
        fig.update_layout(title="Threshold & Model Comparison", barmode="group")
        st.plotly_chart(fig, use_container_width=True)

        # Volatility Trend with Bands
        st.subheader("Historical Volatility & Dynamic Bands")
        trend_df = df[df["Currency"] == selected_currency].copy()
        trend_df = trend_df.sort_values("Date")
        y_actual = trend_df["Volatility"].values
        dates = trend_df["Date"]

        mape_pct = row["Best_MAPE"] / 100 if pd.notnull(row["Best_MAPE"]) else 0.05
        upper_band = y_actual * (1 + mape_pct)
        lower_band = y_actual * (1 - mape_pct)
        flag_mask = (y_actual > upper_band) | (y_actual < lower_band)

        fig_vol = go.Figure()
        fig_vol.add_trace(go.Scatter(x=dates, y=y_actual, name="Actual Volatility", line=dict(color="white")))
        fig_vol.add_trace(go.Scatter(x=dates, y=upper_band, name="Dynamic Upper Band", line=dict(dash="dot", color="green")))
        fig_vol.add_trace(go.Scatter(x=dates, y=lower_band, name="Dynamic Lower Band", line=dict(dash="dot", color="green")))
        fig_vol.add_hline(y=row["ManualThreshold"], line_color="red", line_dash="dot", annotation_text="Manual")
        fig_vol.add_hline(y=row["FinalThreshold"], line_color="orange", line_dash="dash", annotation_text="Final Dynamic")
        fig_vol.add_trace(go.Scatter(
            x=dates[flag_mask],
            y=y_actual[flag_mask],
            mode="markers",
            name="Flagged Deviations",
            marker=dict(color="red", size=8, symbol="x")
        ))
        fig_vol.update_layout(
            title=f"{selected_currency}: Volatility Bands & Threshold Breaches",
            xaxis_title="Date", yaxis_title="Annualized Volatility"
        )
        st.plotly_chart(fig_vol, use_container_width=True)

        # Global RMSE/MAPE chart across currencies
        st.subheader("📊 Global Model Comparison (All Currencies)")
        global_perf = df_final[["Currency", "Best_MAPE", "Model"]].dropna()
        fig_perf = px.bar(global_perf, x="Currency", y="Best_MAPE", color="Model", barmode="group",
                        title="Best Model MAPE per Currency")
        fig_perf.update_layout(yaxis_title="MAPE (%)", xaxis_title="Currency")
        st.plotly_chart(fig_perf, use_container_width=True)

        # Export button
        st.download_button("Download Final Threshold Summary", df_final.to_csv(index=False),
                        file_name="final_fx_threshold_summary.csv")



# --- Tab 5: Cross-Currency Threshold Estimation ---
with tab5:
    st.header("Cross-Currency Thresholds (Synthetic)")

    st.markdown("""
    This module calculates cross-currency thresholds (e.g., AUDCZK = AUDUSD × USDCZK) using historical data.
    - Dynamic threshold = 95th percentile of synthetic volatility.
    - Manual threshold = max(manual thresholds of base and quote legs).
    - Regime flags and threshold breaches shown below.
    """)

    currencies = df["Currency"].unique().tolist()
    cross_pairs = [(f"{b}{q}", b, q) for b in currencies for q in currencies if b != q]
    pair_labels = [f"{p[0]} = {p[1]}USD × USD{p[2]}" for p in cross_pairs]
    pair_choice = st.selectbox("Select Cross Pair", pair_labels)

    selected_pair = cross_pairs[pair_labels.index(pair_choice)]
    pair_name, base_ccy, quote_ccy = selected_pair

    try:
        base_df = df[df["Currency"] == base_ccy][["Date", "Close"]].copy()
        quote_df = df[df["Currency"] == quote_ccy][["Date", "Close"]].copy()

        merged = pd.merge(base_df, quote_df, on="Date", suffixes=(f"_{base_ccy}", f"_{quote_ccy}")).dropna()
        merged["SyntheticRate"] = merged[f"Close_{base_ccy}"] / merged[f"Close_{quote_ccy}"]
        merged["LogReturn"] = np.log(merged["SyntheticRate"]).diff()
        merged["CrossVolatility"] = merged["LogReturn"].rolling(ROLL_WINDOW).std() * ANNUALIZE
        merged.dropna(inplace=True)

        df_thresh = df_summary.set_index("Currency")
        manual_cross = max(df_thresh.loc[base_ccy, "ManualThreshold"], df_thresh.loc[quote_ccy, "ManualThreshold"])
        dynamic_cross = merged["CrossVolatility"].quantile(PCT_THRESHOLD)

        # Breach flags
        merged["Flag"] = merged["CrossVolatility"] > dynamic_cross

        # Plot
        st.subheader(f"{pair_name} Thresholds")
        st.markdown(f"""
        - **Cross Volatility Range**: `{merged['CrossVolatility'].min():.4f}` to `{merged['CrossVolatility'].max():.4f}`
        - **Manual Cross Threshold**: `{manual_cross:.4f}`
        - **Dynamic Cross Threshold (95th pct)**: `{dynamic_cross:.4f}`
        """)

        fig = px.line(merged, x="Date", y="CrossVolatility", title=f"{pair_name} Volatility (Synthetic)")
        fig.add_hline(y=manual_cross, line_color="red", line_dash="dot", annotation_text="Manual")
        fig.add_hline(y=dynamic_cross, line_color="green", line_dash="dash", annotation_text="Dynamic")

        # Add breach flags
        breach_points = merged[merged["Flag"] == True]
        fig.add_trace(go.Scatter(
            x=breach_points["Date"],
            y=breach_points["CrossVolatility"],
            mode="markers",
            name="Threshold Breach",
            marker=dict(color="orange", size=10, symbol="x")
        ))

        st.plotly_chart(fig, use_container_width=True)

    except Exception as e:
        st.error(f"Failed to compute synthetic cross pair volatility: {e}")
